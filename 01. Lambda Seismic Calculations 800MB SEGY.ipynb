{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing Seismic Calculations with On-Demand Lambda Computing\n",
    "\n",
    "This is a workflow where we will work through performing calculations on post-stack seismic data using cloud computing.  \n",
    "\n",
    "We will use this Jupyter notebook to walk through the steps, S3 to store the data, and Lambda to do the processing.\n",
    "\n",
    "The seismic data is streamed from S3 without the need to duplicate any portion of it.  This is ideal for situations where you need to work with large (+100GB) SEGY files that cannot always be loaded into memory and to also avoid the cost of making a duplicate on a local filesystem.  S3 allows you to read specific bytes of data from the file allowing for reading and processing of the traces you need.  Lambda will allow you to parallel process the data and with no need to provision any hardware, you only pay for the time it takes to perform the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "First, lets import all the packages we will need.  Notice that all packages are native to SageMaker and we do not need to install anything extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import struct\n",
    "import pickle\n",
    "import botocore\n",
    "import array as arr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from struct import Struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SEGY\n",
    "We will load the SEGY file from S3 and create a StreamingBody object we can use to stream data into the notebook.  We do not need to copy the file locally.  \n",
    "\n",
    "You should modify these variables to point towards your own S3 bucket and SEGY file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_bucket   = 'equinor-volve-data-village'    # S3 bucket name with input data\n",
    "source_folder   = 'Seismic/ST0202/Stacks'         # Folder path, sometimes referred to as the prefix\n",
    "source_filename = 'ST0202R08_PZ_PSDM_FULL_OFFSET_DEPTH.MIG_FIN.POST_STACK.3D.JS-017534.segy' # Filename\n",
    "\n",
    "# Get the file stream\n",
    "s3 = boto3.resource('s3')\n",
    "segy_obj = s3.Object(source_bucket, f\"{source_folder}/{source_filename}\")\n",
    "segy_stream = segy_obj.get()['Body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Headers\n",
    "Lets read the text and binary headers of the SEGY file.  We are assuming the file follows the SEGY Rev1 standard, which is the case for this file.\n",
    "\n",
    "In this format, the first 3200 bytes are the text header, the next 400 are the binary headers, so we are saving it to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_header_raw = segy_stream.read(3200)\n",
    "bin_header_raw = segy_stream.read(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the text headers.  Headers are in cp500/ebcdic encoding. Here are the first 100 bytes in a raw format, which is not directly readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_header_raw[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets decode it and make it more presentable.  It will be useful to see if there are any notes mentioning if the binary headers are stored in the correct byte locations. It is not uncommon for some critical information to be stored in non-standard byte locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecodeTextHeader(text_header_raw):\n",
    "    text_header = text_header_raw.decode('cp500')\n",
    "    text_header = text_header.replace(\"C 1 \", \"\\nC 1 \")\n",
    "    text_header = text_header.replace(\"C 2 \", \"\\nC 2 \")\n",
    "    text_header = text_header.replace(\"C 3 \", \"\\nC 3 \")\n",
    "    text_header = text_header.replace(\"C 4 \", \"\\nC 4 \")\n",
    "    text_header = text_header.replace(\"C 5 \", \"\\nC 5 \")\n",
    "    text_header = text_header.replace(\"C 6 \", \"\\nC 6 \")\n",
    "    text_header = text_header.replace(\"C 7 \", \"\\nC 7 \")\n",
    "    text_header = text_header.replace(\"C 8 \", \"\\nC 8 \")\n",
    "    text_header = text_header.replace(\"C 9 \", \"\\nC 9 \")\n",
    "    text_header = text_header.replace(\"C10 \", \"\\nC10 \")\n",
    "    text_header = text_header.replace(\"C11 \", \"\\nC11 \")\n",
    "    text_header = text_header.replace(\"C12 \", \"\\nC12 \")\n",
    "    text_header = text_header.replace(\"C13 \", \"\\nC13 \")\n",
    "    text_header = text_header.replace(\"C14 \", \"\\nC14 \")\n",
    "    text_header = text_header.replace(\"C15 \", \"\\nC15 \")\n",
    "    text_header = text_header.replace(\"C16 \", \"\\nC16 \")\n",
    "    text_header = text_header.replace(\"C17 \", \"\\nC17 \")\n",
    "    text_header = text_header.replace(\"C18 \", \"\\nC18 \")\n",
    "    text_header = text_header.replace(\"C19 \", \"\\nC19 \")\n",
    "    text_header = text_header.replace(\"C20 \", \"\\nC20 \")\n",
    "    text_header = text_header.replace(\"C21 \", \"\\nC21 \")\n",
    "    text_header = text_header.replace(\"C22 \", \"\\nC22 \")\n",
    "    text_header = text_header.replace(\"C23 \", \"\\nC23 \")\n",
    "    text_header = text_header.replace(\"C24 \", \"\\nC24 \")\n",
    "    text_header = text_header.replace(\"C25 \", \"\\nC25 \")\n",
    "    text_header = text_header.replace(\"C26 \", \"\\nC26 \")\n",
    "    text_header = text_header.replace(\"C27 \", \"\\nC27 \")\n",
    "    text_header = text_header.replace(\"C28 \", \"\\nC28 \")\n",
    "    text_header = text_header.replace(\"C29 \", \"\\nC29 \")\n",
    "    text_header = text_header.replace(\"C30 \", \"\\nC30 \")\n",
    "    text_header = text_header.replace(\"C31 \", \"\\nC31 \")\n",
    "    text_header = text_header.replace(\"C32 \", \"\\nC32 \")\n",
    "    text_header = text_header.replace(\"C33 \", \"\\nC33 \")\n",
    "    text_header = text_header.replace(\"C34 \", \"\\nC34 \")\n",
    "    text_header = text_header.replace(\"C35 \", \"\\nC35 \")\n",
    "    text_header = text_header.replace(\"C36 \", \"\\nC36 \")\n",
    "    text_header = text_header.replace(\"C37 \", \"\\nC37 \")\n",
    "    text_header = text_header.replace(\"C38 \", \"\\nC38 \")\n",
    "    text_header = text_header.replace(\"C39 \", \"\\nC39 \")\n",
    "    text_header = text_header.replace(\"C40 \", \"\\nC40 \")\n",
    "    \n",
    "    return text_header\n",
    "\n",
    "text_header = DecodeTextHeader(text_header_raw)\n",
    "\n",
    "print(text_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the binary header; it is raw bytes and needs to be decoded too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_header_raw[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets parse all the fields and assign them to the correct variables.  We need to convert bytes to integers, using big-endian byte order.  Most fields are 2 or 4 bytes.  Again, this is following the SEGY Rev1 standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for decoding the binary headers.\n",
    "def DecodeBinHeader(bin_header_raw):\n",
    "    bin_header = {}\n",
    "\n",
    "    bin_header['job_id']                  = int.from_bytes(bin_header_raw[0:4], byteorder='big', signed=False)\n",
    "    bin_header['line_no']                 = int.from_bytes(bin_header_raw[4:8], byteorder='big', signed=False)\n",
    "    bin_header['reel_no']                 = int.from_bytes(bin_header_raw[8:12], byteorder='big', signed=False)\n",
    "    bin_header['data_traces']             = int.from_bytes(bin_header_raw[12:14], byteorder='big', signed=False)\n",
    "    bin_header['aux_traces']              = int.from_bytes(bin_header_raw[14:16], byteorder='big', signed=False)\n",
    "    bin_header['sample_interval']         = int.from_bytes(bin_header_raw[16:18], byteorder='big', signed=False)\n",
    "    bin_header['sample_interval_orig']    = int.from_bytes(bin_header_raw[18:20], byteorder='big', signed=False)\n",
    "    bin_header['samples_per_trace']       = int.from_bytes(bin_header_raw[20:22], byteorder='big', signed=False)\n",
    "    bin_header['samples_per_trace_orig']  = int.from_bytes(bin_header_raw[22:24], byteorder='big', signed=False)\n",
    "    bin_header['data_sample_format']      = int.from_bytes(bin_header_raw[24:26], byteorder='big', signed=False)\n",
    "    bin_header['ensemble_fold']           = int.from_bytes(bin_header_raw[26:28], byteorder='big', signed=False)\n",
    "    bin_header['trace_sorting']           = int.from_bytes(bin_header_raw[28:30], byteorder='big', signed=False)\n",
    "    bin_header['vert_sum_code']           = int.from_bytes(bin_header_raw[30:32], byteorder='big', signed=False)\n",
    "    bin_header['sweep_hz_start']          = int.from_bytes(bin_header_raw[32:34], byteorder='big', signed=False)\n",
    "    bin_header['sweep_hz_end']            = int.from_bytes(bin_header_raw[34:36], byteorder='big', signed=False)\n",
    "    bin_header['sweep_length']            = int.from_bytes(bin_header_raw[36:38], byteorder='big', signed=False)\n",
    "    bin_header['sweep_type']              = int.from_bytes(bin_header_raw[38:40], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_ch']          = int.from_bytes(bin_header_raw[40:42], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_taper_start'] = int.from_bytes(bin_header_raw[42:44], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_taper_end']   = int.from_bytes(bin_header_raw[44:46], byteorder='big', signed=False)\n",
    "    bin_header['taper_type']              = int.from_bytes(bin_header_raw[46:48], byteorder='big', signed=False)\n",
    "    bin_header['data_traces_correlated']  = int.from_bytes(bin_header_raw[48:50], byteorder='big', signed=False)\n",
    "    bin_header['binary_gain_recovered']   = int.from_bytes(bin_header_raw[50:52], byteorder='big', signed=False)\n",
    "    bin_header['amp_recovery_method']     = int.from_bytes(bin_header_raw[52:54], byteorder='big', signed=False)\n",
    "    bin_header['measurement_system']      = int.from_bytes(bin_header_raw[54:56], byteorder='big', signed=False)\n",
    "    bin_header['impulse_sig_polarity']    = int.from_bytes(bin_header_raw[56:58], byteorder='big', signed=False)\n",
    "    bin_header['vib_polarity']            = int.from_bytes(bin_header_raw[58:60], byteorder='big', signed=False)\n",
    "    bin_header['unassigned']              = int.from_bytes(bin_header_raw[60:300], byteorder='big', signed=False)\n",
    "    bin_header['segy_format']             = int.from_bytes(bin_header_raw[300:302], byteorder='big', signed=False)\n",
    "    bin_header['fixed_length_flag']       = int.from_bytes(bin_header_raw[302:304], byteorder='big', signed=False)\n",
    "    bin_header['extended_text_header_no'] = int.from_bytes(bin_header_raw[304:306], byteorder='big', signed=False)\n",
    "    bin_header['unassigned2']             = int.from_bytes(bin_header_raw[306:400], byteorder='big', signed=False)\n",
    "    \n",
    "    return bin_header\n",
    "\n",
    "# Function for printing the binary headers.\n",
    "def PrintBinHeader(bin_header):\n",
    "    print(\"job_id                  = \", bin_header['job_id']                 )\n",
    "    print(\"line_no                 = \", bin_header['line_no']                )\n",
    "    print(\"reel_no                 = \", bin_header['reel_no']                )\n",
    "    print(\"data_traces             = \", bin_header['data_traces']            )\n",
    "    print(\"aux_traces              = \", bin_header['aux_traces']             )\n",
    "    print(\"sample_interval         = \", bin_header['sample_interval']        )\n",
    "    print(\"sample_interval_orig    = \", bin_header['sample_interval_orig']   )\n",
    "    print(\"samples_per_trace       = \", bin_header['samples_per_trace']      )\n",
    "    print(\"samples_per_trace_orig  = \", bin_header['samples_per_trace_orig'] )\n",
    "    print(\"data_sample_format      = \", bin_header['data_sample_format']     )\n",
    "    print(\"ensemble_fold           = \", bin_header['ensemble_fold']          )\n",
    "    print(\"trace_sorting           = \", bin_header['trace_sorting']          )\n",
    "    print(\"vert_sum_code           = \", bin_header['vert_sum_code']          )\n",
    "    print(\"sweep_hz_start          = \", bin_header['sweep_hz_start']         )\n",
    "    print(\"sweep_hz_end            = \", bin_header['sweep_hz_end']           )\n",
    "    print(\"sweep_length            = \", bin_header['sweep_length']           )\n",
    "    print(\"sweep_type              = \", bin_header['sweep_type']             )\n",
    "    print(\"sweep_trace_ch          = \", bin_header['sweep_trace_ch']         )\n",
    "    print(\"sweep_trace_taper_start = \", bin_header['sweep_trace_taper_start'])\n",
    "    print(\"sweep_trace_taper_end   = \", bin_header['sweep_trace_taper_end']  )\n",
    "    print(\"taper_type              = \", bin_header['taper_type']             )\n",
    "    print(\"data_traces_correlated  = \", bin_header['data_traces_correlated'] )\n",
    "    print(\"binary_gain_recovered   = \", bin_header['binary_gain_recovered']  )\n",
    "    print(\"amp_recovery_method     = \", bin_header['amp_recovery_method']    )\n",
    "    print(\"measurement_system      = \", bin_header['measurement_system']     )\n",
    "    print(\"impulse_sig_polarity    = \", bin_header['impulse_sig_polarity']   )\n",
    "    print(\"vib_polarity            = \", bin_header['vib_polarity']           )\n",
    "    print(\"unassigned              = \", bin_header['unassigned']             )\n",
    "    print(\"segy_format             = \", bin(bin_header['segy_format'])[2:]   ) # Refer to SEGY standard on how to read this \n",
    "    print(\"fixed_length_flag       = \", bin_header['fixed_length_flag']      )\n",
    "    print(\"extended_text_header_no = \", bin_header['extended_text_header_no'])\n",
    "    print(\"unassigned2             = \", bin_header['unassigned2']            )\n",
    "    \n",
    "bin_header = DecodeBinHeader(bin_header_raw)\n",
    "PrintBinHeader(bin_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traces\n",
    "After the file headers, the remainder of the SEGY consists of pairs of trace header blocks and the actual trace values in sequence.\n",
    "\n",
    "Lets read the first binary trace header and parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_header_raw = segy_stream.read(240)\n",
    "print(trace_header_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets decode it.  The function below does not read in all the data, but we are pulling what we will be needing for this process: trace number, number of samples, and coordinates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for decoding the trace headers.\n",
    "def DecodeTraceHeader(trace_header_raw):\n",
    "    trace_header = {}\n",
    "    trace_header['trace_seq_no_all']            = int.from_bytes(trace_header_raw[0:4], byteorder='big', signed=False)\n",
    "    trace_header['trace_seq_no_file']           = int.from_bytes(trace_header_raw[4:8], byteorder='big', signed=False)\n",
    "    trace_header['field_record_no_orig']        = int.from_bytes(trace_header_raw[8:12], byteorder='big', signed=False)\n",
    "    trace_header['trace_no_field_orig']         = int.from_bytes(trace_header_raw[12:16], byteorder='big', signed=False)\n",
    "    trace_header['energy_source_point_no']      = int.from_bytes(trace_header_raw[16:20], byteorder='big', signed=False)\n",
    "    trace_header['ensemble_no']                 = int.from_bytes(trace_header_raw[20:24], byteorder='big', signed=False)\n",
    "    trace_header['ensemble_trace_no']           = int.from_bytes(trace_header_raw[24:28], byteorder='big', signed=False)\n",
    "    trace_header['trace_id']                    = int.from_bytes(trace_header_raw[28:30], byteorder='big', signed=False)\n",
    "    trace_header['sum_vertical_traces']         = int.from_bytes(trace_header_raw[30:32], byteorder='big', signed=False)\n",
    "    trace_header['sum_horizontal_traces']       = int.from_bytes(trace_header_raw[32:34], byteorder='big', signed=False)\n",
    "    trace_header['data_use']                    = int.from_bytes(trace_header_raw[34:36], byteorder='big', signed=False)\n",
    "    trace_header['distance_from_source_center'] = int.from_bytes(trace_header_raw[36:40], byteorder='big', signed=False)\n",
    "    # ... incomplete\n",
    "    trace_header['group_x']                     = int.from_bytes(trace_header_raw[80:84], byteorder='big', signed=False)\n",
    "    trace_header['group_y']                     = int.from_bytes(trace_header_raw[84:88], byteorder='big', signed=False)\n",
    "    trace_header['coord_units']                 = int.from_bytes(trace_header_raw[88:90], byteorder='big', signed=False)\n",
    "    trace_header['trace_samples']               = int.from_bytes(trace_header_raw[114:116], byteorder='big', signed=False)\n",
    "\n",
    "    return trace_header\n",
    "    \n",
    "trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "\n",
    "# Function for printing the trace header variables\n",
    "def PrintTraceHeaders(trace_header):\n",
    "    print(\"trace_seq_no_all            = \", trace_header['trace_seq_no_all'])\n",
    "    print(\"trace_seq_no_file           = \", trace_header['trace_seq_no_file'])\n",
    "    print(\"field_record_no_orig        = \", trace_header['field_record_no_orig'])\n",
    "    print(\"trace_no_field_orig         = \", trace_header['trace_no_field_orig'])\n",
    "    print(\"energy_source_point_no      = \", trace_header['energy_source_point_no'])\n",
    "    print(\"ensemble_no                 = \", trace_header['ensemble_no'])\n",
    "    print(\"ensemble_trace_no           = \", trace_header['ensemble_trace_no'])\n",
    "    print(\"trace_id                    = \", trace_header['trace_id'])\n",
    "    print(\"sum_vertical_traces         = \", trace_header['sum_vertical_traces'])\n",
    "    print(\"sum_horizontal_traces       = \", trace_header['sum_horizontal_traces'])\n",
    "    print(\"data_use                    = \", trace_header['data_use'])\n",
    "    print(\"distance_from_source_center = \", trace_header['distance_from_source_center'])\n",
    "    # ... incomplete\n",
    "    print(\"group_x                     = \", trace_header['group_x'])\n",
    "    print(\"group_y                     = \", trace_header['group_y'])\n",
    "    print(\"coord_units                 = \", trace_header['coord_units'])\n",
    "    print(\"trace_samples               = \", trace_header['trace_samples'])\n",
    "\n",
    "\n",
    "PrintTraceHeaders(trace_header)\n",
    "\n",
    "# Incomplete, trace header goes until 240 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the header decoded, lets get the actual trace values that follow it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_raw = segy_stream.read(trace_header['trace_samples']*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEGY stores the trace amplitudes in different formats, outlined by the binary header \"data_sample_format\".  In this case, it is stored as \"IBM 4-byte\", which is not directly usable in Python.  Therefore we will convert it to \"IEEE 4-byte\" for easier consumption.\n",
    "\n",
    "Below is a helper function to convert the trace values. \n",
    "\n",
    "(Code Source: https://stackoverflow.com/questions/7125890/python-unpack-ibm-32-bit-float-point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that converts IBM 4-byte to IEEE\n",
    "class StructIBM32(object):\n",
    "    def __init__(self, size):\n",
    "        self.p24 = float(pow(2, 24))\n",
    "        self.unpack32int = Struct(\">%sL\" % size).unpack\n",
    "    def unpack(self, data):\n",
    "        int32 = self.unpack32int(data)\n",
    "        return [self.ibm2ieee(i) for i in int32]\n",
    "    def ibm2ieee(self, int32):\n",
    "        if int32 == 0:\n",
    "            return 0.0\n",
    "        sign = int32 >> 31 & 0x01\n",
    "        exponent = int32 >> 24 & 0x7f\n",
    "        mantissa = (int32 & 0x00ffffff) / self.p24\n",
    "        return (1 - 2 * sign) * mantissa * pow(16, exponent - 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read in the trace data from the SEGY stream and decode the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_temp = []\n",
    "converter = StructIBM32(1)\n",
    "\n",
    "for x in range(4, trace_header['trace_samples']*4+4, 4):\n",
    "    amp_int = int.from_bytes(trace_raw[x-4:x], byteorder='little', signed=False) # Convert bytes to integers\n",
    "    amp_bin = bin(amp_int)                                                       # Convert integers to binary representation\n",
    "    amplitude = converter.unpack(struct.pack('<L', int(amp_bin, 2)))             # Convert binary to actual IEEE values\n",
    "    trace_temp.append(amplitude[0])\n",
    "\n",
    "trace = np.array(trace_temp)\n",
    "print(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers are nice to see, but hard to tell if they are correctly parsed.  Lets plot the values in Matplotlib to make sure they are correct and look like a seismic wiggle.  There are many visualizations options available in Python and SageMaker, but matplotlib is a good simple choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = np.amax(np.absolute(trace))\n",
    "plt.figure(figsize=(5, 10))\n",
    "plt.plot(trace, range(trace_header['trace_samples']), 'red')\n",
    "plt.xlim(-limits, limits)\n",
    "plt.ylim(trace_header['trace_samples'], 0)\n",
    "plt.axvline(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks correct.  \n",
    "Lets do a simple calculation on the trace: mean amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean absolute amplitude = {:0.5f}\".format(np.mean(np.absolute(trace))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see about processing more traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_amp_mean = {}\n",
    "\n",
    "# Store the first trace result from above\n",
    "trace_amp_mean[1] = np.mean(np.absolute(trace))\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through all the remaining traces\n",
    "current_trace = 0\n",
    "while True:\n",
    "    current_trace = current_trace + 1\n",
    "\n",
    "    trace_header_raw = segy_stream.read(240)\n",
    "    trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "    trace_raw = segy_stream.read(trace_header['trace_samples']*4)\n",
    "\n",
    "    trace = []\n",
    "    converter = StructIBM32(1)\n",
    "\n",
    "    for x in range(4, trace_header['trace_samples']*4, 4):\n",
    "        amplitude = converter.unpack(struct.pack('<L', int(bin(int.from_bytes(trace_raw[x-4:x], byteorder='little', signed=False)), 2)))\n",
    "        trace.append(amplitude)\n",
    "\n",
    "    trace_amp_mean[current_trace] = np.mean(np.absolute(trace))\n",
    "\n",
    "    if current_trace%1000 == 0:\n",
    "        print(\"Trace #{} has mean absolute amplitude of {:.5f}, elapse time is {:.2f} seconds.\".format(current_trace, trace_amp_mean[current_trace], time.time() - start_time))\n",
    "\n",
    "    if current_trace > 10000:\n",
    "        print(\"Stopping here.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slow, especially on a basic EC2 machine and without parallel processing.  Lets clean up the S3 stream and try something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segy_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Out\n",
    "\n",
    "Processing the SEGY file with a single machine is quite slow.  It would take about 18 minutes to complete on this T2.Medium instance.  We could scale up and use a more powerful instance type, but then we are paying per hour, even if we are not using the full compute capacity.  Futhermore will will have to code the parallel processing.  It can quickly start becoming expensive to leave the machines running and the code complexity will increase.  The flow would look like the image below:\n",
    "\n",
    "\n",
    "![title](images/Page-1.png)\n",
    "\n",
    "\n",
    "So lets leverage the power of the cloud to scale out and distribute the processing workload with on-demand processing, Lambda!  Lets split this workload and send it to 100 Lambdas that can run in parallel.  We do not need to send any seismic data to Lambda, we will simply tell each one where to find the file on S3 and which portion of the file to load.\n",
    "\n",
    "\n",
    "![title](images/Page-2.png)\n",
    "\n",
    "\n",
    "If you are following along with the GitHub repo and using the CloudFormation template, put in the bucket name you created when deploying the template.\n",
    "\n",
    "There is a Lambda function that contains the same calculations we performed above called [SegyBatchProcessMeanAmp](https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/SegyBatchProcessMeanAmp).  If you want, go check it out in the AWS Console.  It was deployed with the CF template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_name        = \"SegyBatchProcessMeanAmp\"             # Name of the Lambda function to invoke\n",
    "results_bucket     = \"vavourak-testing\"                    # Bucket to use\n",
    "mean_amp_folder    = \"temp-trace-bundles-ST0202-mean-amp\"  # Subfolder to place the calculation results\n",
    "concurrent_lambdas = 100                                   # Number of Lambdas to invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get S3 object\n",
    "segy_obj = s3.Object(source_bucket, f\"{source_folder}/{source_filename}\")\n",
    "\n",
    "# Define some needed variables based off the above parameters\n",
    "start_time = time.time()\n",
    "header_size = 3600\n",
    "trace_header_size = 240\n",
    "trace_size = bin_header['samples_per_trace'] * 4\n",
    "trace_size_with_headers = trace_size + trace_header_size\n",
    "filesize = segy_obj.content_length\n",
    "trace_count = int((filesize - 3600) / trace_size_with_headers)\n",
    "bundle_size = round(trace_count/concurrent_lambdas)\n",
    "\n",
    "results_file_list = [] # Lets keep track of the output file names, so we can grab them later\n",
    "\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "print(f\"Total traces in file: {trace_count}\")\n",
    "print(f\"Traces per Lambda for {concurrent_lambdas} concurrency (not rounded): {trace_count/concurrent_lambdas}\")\n",
    "print(f\"Traces per Lambda, rounded up: {round(trace_count/concurrent_lambdas+0.5)}\")\n",
    "\n",
    "# Send the trace bundle information over to Lambda\n",
    "for bundle in range(0, int(trace_count), bundle_size):\n",
    "    bytes_start = bundle * trace_size_with_headers + header_size\n",
    "    bytes_stop = (bundle + bundle_size) * trace_size_with_headers + header_size - 1\n",
    "    #print(f\"Bundle traces: {bundle}-{bundle+bundle_size}, bytes: {bytes_start}-{bytes_stop}.\")\n",
    "    \n",
    "    # Build the message for the Lambda to find the seismic file\n",
    "    payload = {\n",
    "        \"bucket_in\"          : source_bucket,\n",
    "        \"folder_in\"          : source_folder,\n",
    "        \"filename_in\"        : source_filename,\n",
    "        \"bucket_out\"         : results_bucket,\n",
    "        \"folder_out\"         : mean_amp_folder,\n",
    "        \"bytes_start\"        : bytes_start,\n",
    "        \"bytes_stop\"         : bytes_stop,\n",
    "        \"use_custom_lines\"   : 0,\n",
    "        \"data_sample_format\" : bin_header['data_sample_format']\n",
    "    }\n",
    "\n",
    "    # Invoke the Lambda SegyBatchProcessMeanAmp\n",
    "    response = lambda_client.invoke(FunctionName=lambda_name,\n",
    "                                    InvocationType='Event',\n",
    "                                    Payload=json.dumps(payload))\n",
    "    \n",
    "    results_file_list.append(f\"{mean_amp_folder}/{source_filename}.{bytes_start}-{bytes_stop}.pkl\")\n",
    "\n",
    "print(\"Done!  Elapse time to gather trace info and send to Lambda: {:0.2f} seconds.  Now waiting for 80 seconds.\".format(time.time() - start_time))\n",
    "\n",
    "time.sleep(80)      # Waiting before carrying on next steps, to allow time for Lambda to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The trace bundles are all being processed in parallel by Lambda.  It should take about 1 minute to complete instead of 18!  With Lambda, we do not need to provision any resources or have servers waiting with spare compute capacity.  It is all managed in the background by AWS and you only pay for the compute time needed to run the code.  The cost to do the calculation on this 800MB SEGY file is only $0.02.\n",
    "\n",
    "Here we are not using the full abilities of AWS.  Instead of simply waiting for the Lambdas to complete, we could tie it all together with AWS Step Functions, were we can trigger events, like other Lambdas and responses once a workflow is done.  But we are keeping it simple for this demo and waiting until the process is done with a sleep statement.\n",
    "\n",
    "The Lambdas should be done by now.  Lets load in the results from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "trace_mean_amp = []\n",
    "trace_x = []\n",
    "trace_y = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through the files\n",
    "for x in range(0, len(results_file_list)):\n",
    "    print(\"Reading file: \", results_file_list[x])\n",
    "    \n",
    "    # Get file from S3, convert from Pickle format\n",
    "    object = s3_client.get_object(Bucket=results_bucket, Key=results_file_list[x])\n",
    "    serializedObject = object['Body'].read()\n",
    "    trace_bundle_temp = pickle.loads(serializedObject)\n",
    "    \n",
    "    # Split the tuple [1,2,3] into seperate variables for easier use\n",
    "    for y in range(0, len(trace_bundle_temp)):\n",
    "        trace_mean_amp.append(trace_bundle_temp[y][0])\n",
    "        trace_x.append(trace_bundle_temp[y][1])\n",
    "        trace_y.append(trace_bundle_temp[y][2])\n",
    "\n",
    "print(\"Number of traces loaded: {}, elapsed time: {:0.2f} seconds.\".format(len(trace_mean_amp), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets map out the mean amplitudes using a Matplotlib scatter plot.  The Lambda saved the mean amplitude, along with the X and Y coordinates of each trace group for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(trace_x, trace_y, c=trace_mean_amp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!  Seems like there is some banding along the acquisition lines.  Maybe in the future we can make a Lambda function to balance out the variations along each inline/xline.\n",
    "\n",
    "\n",
    "### Clean-up\n",
    "Lets clean up the files we created.  We could keep them and catalog them in DynamoDB for future use in a production environment so we do not need rerun calculations, but for today we no longer need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(results_file_list)):\n",
    "    _ = s3_client.delete_object(Bucket=results_bucket, Key=results_file_list[x])\n",
    "    print(\"Deleting: {}\".format(results_file_list[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "Lets scale this project out by 2 orders of magnitude!  Instead of processing only 800MB, which is no real feat of engineering and can be done in-memory even on a cell phone these days, we will process a massive 180GB pre-stack raw seismic file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
