{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Scale Distributed Seismic Processing\n",
    "\n",
    "In this notebook we will work with a much larger dataset.  Here we have a 180GB SEGY file containing raw pre-stack seismic data.  Lets take a look at this dataset and then do the same mean amplitude calculations, using only a basic T3.Medium instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import struct\n",
    "import pickle\n",
    "import botocore\n",
    "import array as arr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from struct import Struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "We will reuse the SEGY loading functions from before, so lets define them all in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecodeTextHeader(text_header_raw):\n",
    "    text_header = text_header_raw.decode('cp500')\n",
    "    text_header = text_header.replace(\"C 1 \", \"\\nC 1 \")\n",
    "    text_header = text_header.replace(\"C 2 \", \"\\nC 2 \")\n",
    "    text_header = text_header.replace(\"C 3 \", \"\\nC 3 \")\n",
    "    text_header = text_header.replace(\"C 4 \", \"\\nC 4 \")\n",
    "    text_header = text_header.replace(\"C 5 \", \"\\nC 5 \")\n",
    "    text_header = text_header.replace(\"C 6 \", \"\\nC 6 \")\n",
    "    text_header = text_header.replace(\"C 7 \", \"\\nC 7 \")\n",
    "    text_header = text_header.replace(\"C 8 \", \"\\nC 8 \")\n",
    "    text_header = text_header.replace(\"C 9 \", \"\\nC 9 \")\n",
    "    text_header = text_header.replace(\"C10 \", \"\\nC10 \")\n",
    "    text_header = text_header.replace(\"C11 \", \"\\nC11 \")\n",
    "    text_header = text_header.replace(\"C12 \", \"\\nC12 \")\n",
    "    text_header = text_header.replace(\"C13 \", \"\\nC13 \")\n",
    "    text_header = text_header.replace(\"C14 \", \"\\nC14 \")\n",
    "    text_header = text_header.replace(\"C15 \", \"\\nC15 \")\n",
    "    text_header = text_header.replace(\"C16 \", \"\\nC16 \")\n",
    "    text_header = text_header.replace(\"C17 \", \"\\nC17 \")\n",
    "    text_header = text_header.replace(\"C18 \", \"\\nC18 \")\n",
    "    text_header = text_header.replace(\"C19 \", \"\\nC19 \")\n",
    "    text_header = text_header.replace(\"C20 \", \"\\nC20 \")\n",
    "    text_header = text_header.replace(\"C21 \", \"\\nC21 \")\n",
    "    text_header = text_header.replace(\"C22 \", \"\\nC22 \")\n",
    "    text_header = text_header.replace(\"C23 \", \"\\nC23 \")\n",
    "    text_header = text_header.replace(\"C24 \", \"\\nC24 \")\n",
    "    text_header = text_header.replace(\"C25 \", \"\\nC25 \")\n",
    "    text_header = text_header.replace(\"C26 \", \"\\nC26 \")\n",
    "    text_header = text_header.replace(\"C27 \", \"\\nC27 \")\n",
    "    text_header = text_header.replace(\"C28 \", \"\\nC28 \")\n",
    "    text_header = text_header.replace(\"C29 \", \"\\nC29 \")\n",
    "    text_header = text_header.replace(\"C30 \", \"\\nC30 \")\n",
    "    text_header = text_header.replace(\"C31 \", \"\\nC31 \")\n",
    "    text_header = text_header.replace(\"C32 \", \"\\nC32 \")\n",
    "    text_header = text_header.replace(\"C33 \", \"\\nC33 \")\n",
    "    text_header = text_header.replace(\"C34 \", \"\\nC34 \")\n",
    "    text_header = text_header.replace(\"C35 \", \"\\nC35 \")\n",
    "    text_header = text_header.replace(\"C36 \", \"\\nC36 \")\n",
    "    text_header = text_header.replace(\"C37 \", \"\\nC37 \")\n",
    "    text_header = text_header.replace(\"C38 \", \"\\nC38 \")\n",
    "    text_header = text_header.replace(\"C39 \", \"\\nC39 \")\n",
    "    text_header = text_header.replace(\"C40 \", \"\\nC40 \")\n",
    "    \n",
    "    return text_header\n",
    "\n",
    "\n",
    "def DecodeBinHeader(bin_header_raw):\n",
    "    bin_header = {}\n",
    "\n",
    "    bin_header['job_id']                  = int.from_bytes(bin_header_raw[0:4], byteorder='big', signed=False)\n",
    "    bin_header['line_no']                 = int.from_bytes(bin_header_raw[4:8], byteorder='big', signed=False)\n",
    "    bin_header['reel_no']                 = int.from_bytes(bin_header_raw[8:12], byteorder='big', signed=False)\n",
    "    bin_header['data_traces']             = int.from_bytes(bin_header_raw[12:14], byteorder='big', signed=False)\n",
    "    bin_header['aux_traces']              = int.from_bytes(bin_header_raw[14:16], byteorder='big', signed=False)\n",
    "    bin_header['sample_interval']         = int.from_bytes(bin_header_raw[16:18], byteorder='big', signed=False)\n",
    "    bin_header['sample_interval_orig']    = int.from_bytes(bin_header_raw[18:20], byteorder='big', signed=False)\n",
    "    bin_header['samples_per_trace']       = int.from_bytes(bin_header_raw[20:22], byteorder='big', signed=False)\n",
    "    bin_header['samples_per_trace_orig']  = int.from_bytes(bin_header_raw[22:24], byteorder='big', signed=False)\n",
    "    bin_header['data_sample_format']      = int.from_bytes(bin_header_raw[24:26], byteorder='big', signed=False)\n",
    "    bin_header['ensemble_fold']           = int.from_bytes(bin_header_raw[26:28], byteorder='big', signed=False)\n",
    "    bin_header['trace_sorting']           = int.from_bytes(bin_header_raw[28:30], byteorder='big', signed=False)\n",
    "    bin_header['vert_sum_code']           = int.from_bytes(bin_header_raw[30:32], byteorder='big', signed=False)\n",
    "    bin_header['sweep_hz_start']          = int.from_bytes(bin_header_raw[32:34], byteorder='big', signed=False)\n",
    "    bin_header['sweep_hz_end']            = int.from_bytes(bin_header_raw[34:36], byteorder='big', signed=False)\n",
    "    bin_header['sweep_length']            = int.from_bytes(bin_header_raw[36:38], byteorder='big', signed=False)\n",
    "    bin_header['sweep_type']              = int.from_bytes(bin_header_raw[38:40], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_ch']          = int.from_bytes(bin_header_raw[40:42], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_taper_start'] = int.from_bytes(bin_header_raw[42:44], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_taper_end']   = int.from_bytes(bin_header_raw[44:46], byteorder='big', signed=False)\n",
    "    bin_header['taper_type']              = int.from_bytes(bin_header_raw[46:48], byteorder='big', signed=False)\n",
    "    bin_header['data_traces_correlated']  = int.from_bytes(bin_header_raw[48:50], byteorder='big', signed=False)\n",
    "    bin_header['binary_gain_recovered']   = int.from_bytes(bin_header_raw[50:52], byteorder='big', signed=False)\n",
    "    bin_header['amp_recovery_method']     = int.from_bytes(bin_header_raw[52:54], byteorder='big', signed=False)\n",
    "    bin_header['measurement_system']      = int.from_bytes(bin_header_raw[54:56], byteorder='big', signed=False)\n",
    "    bin_header['impulse_sig_polarity']    = int.from_bytes(bin_header_raw[56:58], byteorder='big', signed=False)\n",
    "    bin_header['vib_polarity']            = int.from_bytes(bin_header_raw[58:60], byteorder='big', signed=False)\n",
    "    bin_header['unassigned']              = int.from_bytes(bin_header_raw[60:300], byteorder='big', signed=False)\n",
    "    bin_header['segy_format']             = int.from_bytes(bin_header_raw[300:302], byteorder='big', signed=False)\n",
    "    bin_header['fixed_length_flag']       = int.from_bytes(bin_header_raw[302:304], byteorder='big', signed=False)\n",
    "    bin_header['extended_text_header_no'] = int.from_bytes(bin_header_raw[304:306], byteorder='big', signed=False)\n",
    "    bin_header['unassigned2']             = int.from_bytes(bin_header_raw[306:400], byteorder='big', signed=False)\n",
    "    \n",
    "    return bin_header\n",
    "\n",
    "\n",
    "def PrintBinHeader(bin_header):\n",
    "    print(\"job_id                  = \", bin_header['job_id']                 )\n",
    "    print(\"line_no                 = \", bin_header['line_no']                )\n",
    "    print(\"reel_no                 = \", bin_header['reel_no']                )\n",
    "    print(\"data_traces             = \", bin_header['data_traces']            )\n",
    "    print(\"aux_traces              = \", bin_header['aux_traces']             )\n",
    "    print(\"sample_interval         = \", bin_header['sample_interval']        )\n",
    "    print(\"sample_interval_orig    = \", bin_header['sample_interval_orig']   )\n",
    "    print(\"samples_per_trace       = \", bin_header['samples_per_trace']      )\n",
    "    print(\"samples_per_trace_orig  = \", bin_header['samples_per_trace_orig'] )\n",
    "    print(\"data_sample_format      = \", bin_header['data_sample_format']     )\n",
    "    print(\"ensemble_fold           = \", bin_header['ensemble_fold']          )\n",
    "    print(\"trace_sorting           = \", bin_header['trace_sorting']          )\n",
    "    print(\"vert_sum_code           = \", bin_header['vert_sum_code']          )\n",
    "    print(\"sweep_hz_start          = \", bin_header['sweep_hz_start']         )\n",
    "    print(\"sweep_hz_end            = \", bin_header['sweep_hz_end']           )\n",
    "    print(\"sweep_length            = \", bin_header['sweep_length']           )\n",
    "    print(\"sweep_type              = \", bin_header['sweep_type']             )\n",
    "    print(\"sweep_trace_ch          = \", bin_header['sweep_trace_ch']         )\n",
    "    print(\"sweep_trace_taper_start = \", bin_header['sweep_trace_taper_start'])\n",
    "    print(\"sweep_trace_taper_end   = \", bin_header['sweep_trace_taper_end']  )\n",
    "    print(\"taper_type              = \", bin_header['taper_type']             )\n",
    "    print(\"data_traces_correlated  = \", bin_header['data_traces_correlated'] )\n",
    "    print(\"binary_gain_recovered   = \", bin_header['binary_gain_recovered']  )\n",
    "    print(\"amp_recovery_method     = \", bin_header['amp_recovery_method']    )\n",
    "    print(\"measurement_system      = \", bin_header['measurement_system']     )\n",
    "    print(\"impulse_sig_polarity    = \", bin_header['impulse_sig_polarity']   )\n",
    "    print(\"vib_polarity            = \", bin_header['vib_polarity']           )\n",
    "    print(\"unassigned              = \", bin_header['unassigned']             )\n",
    "    print(\"segy_format             = \", bin(bin_header['segy_format'])[2:]   )    \n",
    "    print(\"fixed_length_flag       = \", bin_header['fixed_length_flag']      )\n",
    "    print(\"extended_text_header_no = \", bin_header['extended_text_header_no'])\n",
    "    print(\"unassigned2             = \", bin_header['unassigned2']            )\n",
    "    \n",
    "    \n",
    "def DecodeTraceHeader(trace_header_raw):\n",
    "    trace_header = {}\n",
    "    trace_header['trace_seq_no_all']            = int.from_bytes(trace_header_raw[0:4], byteorder='big', signed=False)\n",
    "    trace_header['trace_seq_no_file']           = int.from_bytes(trace_header_raw[4:8], byteorder='big', signed=False)\n",
    "    trace_header['field_record_no_orig']        = int.from_bytes(trace_header_raw[8:12], byteorder='big', signed=False)\n",
    "    trace_header['trace_no_field_orig']         = int.from_bytes(trace_header_raw[12:16], byteorder='big', signed=False)\n",
    "    trace_header['energy_source_point_no']      = int.from_bytes(trace_header_raw[16:20], byteorder='big', signed=False)\n",
    "    trace_header['ensemble_no']                 = int.from_bytes(trace_header_raw[20:24], byteorder='big', signed=False)\n",
    "    trace_header['ensemble_trace_no']           = int.from_bytes(trace_header_raw[24:28], byteorder='big', signed=False)\n",
    "    trace_header['trace_id']                    = int.from_bytes(trace_header_raw[28:30], byteorder='big', signed=False)\n",
    "    trace_header['sum_vertical_traces']         = int.from_bytes(trace_header_raw[30:32], byteorder='big', signed=False)\n",
    "    trace_header['sum_horizontal_traces']       = int.from_bytes(trace_header_raw[32:34], byteorder='big', signed=False)\n",
    "    trace_header['data_use']                    = int.from_bytes(trace_header_raw[34:36], byteorder='big', signed=False)\n",
    "    trace_header['distance_from_source_center'] = int.from_bytes(trace_header_raw[36:40], byteorder='big', signed=False)\n",
    "    # ... incomplete\n",
    "    trace_header['group_x']                     = int.from_bytes(trace_header_raw[80:84], byteorder='big', signed=False)\n",
    "    trace_header['group_y']                     = int.from_bytes(trace_header_raw[84:88], byteorder='big', signed=False)\n",
    "    trace_header['coord_units']                 = int.from_bytes(trace_header_raw[88:90], byteorder='big', signed=False)\n",
    "    trace_header['trace_samples']               = int.from_bytes(trace_header_raw[114:116], byteorder='big', signed=False)\n",
    "    trace_header['sample_interval']             = int.from_bytes(trace_header_raw[116:118], byteorder='big', signed=False)\n",
    "    trace_header['gain_type']                   = int.from_bytes(trace_header_raw[118:120], byteorder='big', signed=False)\n",
    "    trace_header['CDP_X']                       = int.from_bytes(trace_header_raw[180:184], byteorder='big', signed=False)\n",
    "    trace_header['CDP_Y']                       = int.from_bytes(trace_header_raw[184:188], byteorder='big', signed=False)\n",
    "    trace_header['inline']                      = int.from_bytes(trace_header_raw[188:192], byteorder='big', signed=False)\n",
    "    trace_header['xline']                       = int.from_bytes(trace_header_raw[192:196], byteorder='big', signed=False)\n",
    "    trace_header['trace_unit']                  = int.from_bytes(trace_header_raw[202:204], byteorder='big', signed=False) \n",
    "    trace_header['inline_custom']               = int.from_bytes(trace_header_raw[232:236], byteorder='big', signed=False)\n",
    "    trace_header['xline_custom']                = int.from_bytes(trace_header_raw[236:240], byteorder='big', signed=False)\n",
    "\n",
    "    return trace_header\n",
    "    \n",
    "\n",
    "def PrintTraceHeaders(trace_header):\n",
    "    print(\"trace_seq_no_all            = \", trace_header['trace_seq_no_all'])\n",
    "    print(\"trace_seq_no_file           = \", trace_header['trace_seq_no_file'])\n",
    "    print(\"field_record_no_orig        = \", trace_header['field_record_no_orig'])\n",
    "    print(\"trace_no_field_orig         = \", trace_header['trace_no_field_orig'])\n",
    "    print(\"energy_source_point_no      = \", trace_header['energy_source_point_no'])\n",
    "    print(\"ensemble_no                 = \", trace_header['ensemble_no'])\n",
    "    print(\"ensemble_trace_no           = \", trace_header['ensemble_trace_no'])\n",
    "    print(\"trace_id                    = \", trace_header['trace_id'])\n",
    "    print(\"sum_vertical_traces         = \", trace_header['sum_vertical_traces'])\n",
    "    print(\"sum_horizontal_traces       = \", trace_header['sum_horizontal_traces'])\n",
    "    print(\"data_use                    = \", trace_header['data_use'])\n",
    "    print(\"distance_from_source_center = \", trace_header['distance_from_source_center'])\n",
    "    # ... incomplete\n",
    "    print(\"group_x                     = \", trace_header['group_x'])\n",
    "    print(\"group_y                     = \", trace_header['group_y'])\n",
    "    print(\"coord_units                 = \", trace_header['coord_units'])\n",
    "    print(\"trace_samples               = \", trace_header['trace_samples'])\n",
    "    print(\"sample_interval             = \", trace_header['sample_interval'])\n",
    "    print(\"gain_type                   = \", trace_header['gain_type'])\n",
    "    print(\"CDP_X                       = \", trace_header['CDP_X'])          \n",
    "    print(\"CDP_Y                       = \", trace_header['CDP_Y'])          \n",
    "    print(\"inline                      = \", trace_header['inline'])         \n",
    "    print(\"xline                       = \", trace_header['xline'])          \n",
    "    print(\"trace_unit                  = \", trace_header['trace_unit'])     \n",
    "    print(\"inline_custom               = \", trace_header['inline_custom'])         \n",
    "    print(\"xline_custom                = \", trace_header['xline_custom'])          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SEGY File\n",
    "\n",
    "Lets load in the large 180GB SEGY file as a data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the 180GB raw pre-stack SEGY data\n",
    "\n",
    "source_bucket   = 'equinor-volve-data-village'                  # S3 bucket name with input data\n",
    "source_folder   = 'Seismic/ST10010/Raw_data/ST10010+NAV_MERGE'  # Folder path\n",
    "source_filename = 'ST10010_1150780_40203.sgy'                   # Filename\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "segy_obj = s3.Object(source_bucket, f\"{source_folder}/{source_filename}\")\n",
    "segy_stream = segy_obj.get()['Body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Headers\n",
    "\n",
    "Lets read the text and binary headers.  We will see that this file does not follow SEGY revision 1 standard as before and has custom trace header byte locations for some information.  We will not be using that data for this notebook.  If we did, we would need to adapt our function above to query the correct locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_header_raw = segy_stream.read(3200)\n",
    "bin_header_raw = segy_stream.read(400)\n",
    "\n",
    "text_header = DecodeTextHeader(text_header_raw)\n",
    "\n",
    "print(\"Text Header:\")\n",
    "print(text_header)\n",
    "\n",
    "bin_header = DecodeBinHeader(bin_header_raw)\n",
    "print(\"\\nBinary Header:\")\n",
    "PrintBinHeader(bin_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace Headers\n",
    "\n",
    "Reading the first trace header.  Notice CDP X/Y and inline/xline values are not correct, as they are stored at another byte location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_header_raw = segy_stream.read(240)\n",
    "\n",
    "trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "PrintTraceHeaders(trace_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace Data\n",
    "\n",
    "Lets read in the first trace and plot it.  In this file the trace data is in IEEE 4-byte float, which is useable in Python.  We do not need to convert it, but only unpack it from the byte format.\n",
    "\n",
    "This raw data looks very different than the processed data we saw before, as it does not have any amplitude gain applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_raw = segy_stream.read(trace_header['trace_samples']*4)\n",
    "\n",
    "trace = []\n",
    "for x in range(4, trace_header['trace_samples']*4+4, 4):\n",
    "    trace.append(struct.unpack('>f', trace_raw[x-4:x])[0])\n",
    "\n",
    "# Lets plot the trace\n",
    "limits = np.amax(np.absolute(trace))\n",
    "plt.figure(figsize=(5, 10))\n",
    "plt.plot(trace, range(trace_header['trace_samples']), 'red')\n",
    "plt.xlim(-limits, limits)\n",
    "plt.ylim(trace_header['trace_samples'], 0)\n",
    "plt.axvline(0, c='grey')\n",
    "\n",
    "\n",
    "segy_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data\n",
    "\n",
    "Great, we are loading the data correctly.  Now to scale this out across multiple Lambdas.\n",
    "\n",
    "First, for a benchmark, lets start processing here and see how long it might take.  Keep in mind we are using a small T2.Medium instance and only using 1 CPU core, as we are not threading the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segy_stream = segy_obj.get()['Body']\n",
    "\n",
    "text_header_raw = segy_stream.read(3200)\n",
    "bin_header_raw = segy_stream.read(400)\n",
    "text_header = DecodeTextHeader(text_header_raw)\n",
    "bin_header = DecodeBinHeader(bin_header_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_amp_mean = {}\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through all the traces\n",
    "while True:\n",
    "    trace_header_raw = segy_stream.read(240)\n",
    "    trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "    trace_raw = segy_stream.read(trace_header['trace_samples']*4)\n",
    "\n",
    "    trace = []\n",
    "    for x in range(4, trace_header['trace_samples']*4+4, 4):\n",
    "        trace.append(struct.unpack('>f', trace_raw[x-4:x])[0])\n",
    "    \n",
    "    trace_amp_mean[trace_header['trace_seq_no_all']] = np.mean(np.absolute(trace))\n",
    "    \n",
    "    if trace_header['trace_seq_no_all']%1000 == 0:\n",
    "        print(\"Trace #{} has a mean amplitude of {}, elapse time is {:.2f} seconds.\".format(\n",
    "                                        trace_header['trace_seq_no_all'], \n",
    "                                        trace_amp_mean[trace_header['trace_seq_no_all']], \n",
    "                                        time.time() - start_time))\n",
    "        \n",
    "    if trace_header['trace_seq_no_all'] > 5000:\n",
    "        print(\"Stopping here.\")\n",
    "        break\n",
    "\n",
    "segy_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Out\n",
    "\n",
    "So thats about 2-3 seconds per 1,000 traces.  Problem is that there are about 9,300,000 traces in this file.  That would take about 6 hours.  Plus the entire raw dataset for the seismic cube contains many such 180GB files.\n",
    "\n",
    "We could scale up and use a 16xLarge instance (64 CPUs) and start threading the calculations.  This might take then take about 6 mins to process, but now we have have file bandwidth bottlenecks.  It takes about 75 minutes to read through the file from S3 to this notebook from end to end.  Further downside is that you have to do the undiferentiated heavy lifting of provisioning the hardware resources for the notebook and starting/stopping them as needed to save costs.  The T3.16xLarge instance cost is about \\\\$3/hour, versus $0.03/hour for our T2.Medium.\n",
    "\n",
    "So lets use Lambda again to process the file.  No need for provisioning resources.  With our current default limits, we can have 1000 concurrent Lambdas running, so we will use them all!  This limit can be increased if needed (and should, if doing seismic processing like this).\n",
    "\n",
    "Lets get things setup up below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bucket     = \"vavourak-demo-temp\"                  # Bucket to use, created in the CloudFormation template\n",
    "mean_amp_folder    = \"temp-trace-bundles-st10010-mean-amp\" # Subfolder to place the calculation results\n",
    "lambda_name        = \"SegyBatchProcessMeanAmp\"             # Name of the Lambda function to invoke\n",
    "concurrent_lambdas = 1000                                  # Number of Lambdas to invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split up the SEGY file into byte ranges and invoke 1000 Lambda functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get S3 object\n",
    "segy_obj = s3.Object(source_bucket, f\"{source_folder}/{source_filename}\")\n",
    "\n",
    "# Define some needed variables based off the above parameters\n",
    "start_time = time.time()\n",
    "header_size = 3600\n",
    "trace_header_size = 240\n",
    "trace_size = bin_header['samples_per_trace'] * 4\n",
    "trace_size_with_headers = trace_size + trace_header_size\n",
    "filesize = segy_obj.content_length\n",
    "trace_count = int((filesize - 3600) / trace_size_with_headers)\n",
    "bundle_size = round(trace_count/concurrent_lambdas)\n",
    "lambda_counter = 0\n",
    "\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "results_file_list = [] # Lets keep track of the output file names, so we can grab them later\n",
    "\n",
    "print(f\"Total traces in file: {trace_count}\")\n",
    "print(f\"Traces per Lambda for {concurrent_lambdas} concurrency (not rounded): {trace_count/concurrent_lambdas}\")\n",
    "print(f\"Traces per Lambda, rounded up: {round(trace_count/concurrent_lambdas+0.5)}\")\n",
    "\n",
    "# Send the trace bundle information over to Lambda\n",
    "for bundle in range(0, int(trace_count), bundle_size):\n",
    "    lambda_counter = lambda_counter + 1\n",
    "    bytes_start = bundle * trace_size_with_headers + header_size\n",
    "    bytes_stop = (bundle + bundle_size) * trace_size_with_headers + header_size - 1\n",
    "    print(f\"Lambda #{lambda_counter}, bundled traces: {bundle}-{bundle+bundle_size}, bytes: {bytes_start}-{bytes_stop}.\")\n",
    "    \n",
    "    # Build the message for the Lambda to find the files\n",
    "    payload = {\n",
    "        \"bucket_in\"          : source_bucket,\n",
    "        \"folder_in\"          : source_folder,\n",
    "        \"filename_in\"        : source_filename,\n",
    "        \"bucket_out\"         : results_bucket,\n",
    "        \"folder_out\"         : mean_amp_folder,\n",
    "        \"bytes_start\"        : bytes_start,\n",
    "        \"bytes_stop\"         : bytes_stop,\n",
    "        \"use_custom_lines\"   : 1,\n",
    "        \"data_sample_format\" : bin_header['data_sample_format']\n",
    "    }\n",
    "\n",
    "    # Invoke the Lambda SegyBatchProcessMeanAmp\n",
    "    response = lambda_client.invoke(FunctionName=lambda_name,\n",
    "                                    InvocationType='Event',\n",
    "                                    Payload=json.dumps(payload))\n",
    "\n",
    "    results_file_list.append(f\"{mean_amp_folder}/{source_filename}.{bytes_start}-{bytes_stop}.pkl\")\n",
    "    \n",
    "print(\"Done!  Elapse time to gather traces and send to Lambda: {:0.2f} seconds, now waiting a bit for processing to complete.\".format(time.time() - start_time))\n",
    "\n",
    "time.sleep(400)      # Waiting before carrying on next steps, to allow time for Lambda to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a 1000 Lambdas, taking roughly 330 seconds each, 60 seconds to start them, our total time is about 6.5 minutes.  The cost to perform this calculation on this 180GB file is about $0.67.  As it takes 60 seconds to invoke the Lambdas, there is some room for optimization here to invoke them faster.  Having more Lambda concurrency available will help too.\n",
    "\n",
    "### Load Results\n",
    "The Lambdas should be done by now. Lets load in the results from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "traces = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through the files\n",
    "for x in range(0, len(results_file_list)):\n",
    "    print(\"Reading file: \", results_file_list[x])\n",
    "    \n",
    "    # Get file from S3, convert from Pickle format\n",
    "    object = s3_client.get_object(Bucket=results_bucket, Key=results_file_list[x])\n",
    "    serializedObject = object['Body'].read()\n",
    "    trace_bundle_temp = pickle.loads(serializedObject)\n",
    "    \n",
    "    for y in range(0, len(trace_bundle_temp)):\n",
    "        traces.append(trace_bundle_temp[y])\n",
    "\n",
    "print(\"Number of traces loaded: {}, elapsed time: {:0.2f} seconds.\".format(len(trace_mean_amp), time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting over 9 million traces will be taxing for Matplotlib.  Lets only show the max mean amplitude for each inline-xline location.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_grouped = {}\n",
    "traces_max = {}\n",
    "\n",
    "# Go through every trace and make a dictionary with the values at each unique inline/xline location\n",
    "counter = 0\n",
    "for trace in traces:\n",
    "    counter = counter + 1\n",
    "    group = f\"{trace[1]}-{trace[2]}\"\n",
    "    \n",
    "    if group in traces_grouped:\n",
    "        traces_grouped[str(group)].append(trace[0])\n",
    "    else:\n",
    "        traces_grouped[str(group)] = [trace[0]]\n",
    "\n",
    "# Find the max amplitude at each location\n",
    "for group in traces_grouped:\n",
    "    traces_max[group] = np.amax(traces_grouped[group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary back to arrays for easy consuption\n",
    "traces_max_value = []\n",
    "traces_max_inline = []\n",
    "traces_max_xline = []\n",
    "\n",
    "counter = 0\n",
    "for group in traces_max:\n",
    "    inline = int(group.split(\"-\")[0])\n",
    "    xline = int(group.split(\"-\")[1])\n",
    "    \n",
    "    traces_max_inline.append(inline)\n",
    "    traces_max_xline.append(xline)\n",
    "    traces_max_value.append(traces_max[group])\n",
    "    counter = counter + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets map out the mean amplitudes using a Matplotlib scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 10))\n",
    "plt.scatter(traces_max_inline, traces_max_xline, c=traces_max_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We have successfully performed a calculation on a 180GB seismic SEGY file without having to spin up or down a single computer, minus this notebook.  All at the cost of $0.67 and 6.5 minutes of processing time.  This process is scalable and repeatable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Lets clean up the files we generated, so that we can delete the CloudFormation stack without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(results_file_list)):\n",
    "    _ = s3_client.delete_object(Bucket=results_bucket, Key=results_file_list[x])\n",
    "    print(\"Deleting: {}\".format(results_file_list[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
