{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing Seismic Processing with On-Demand Computing\n",
    "\n",
    "This is a workflow where we will work through performing calculations on post-stack seismic data using cloud computing.  \n",
    "\n",
    "We will use this Jupyter notebook to walk through the steps (the only provisioned resource used), S3 to store the data, and Lambda to do the processing.\n",
    "\n",
    "The seismic data is streamed from S3 without the need to duplicate any portion of it.  This is ideal for situations where you need to work with large (+100GB) SEGY files that cannot always be loaded into memory and to also avoid the cost of making a duplicate on a local filesystem.  Lambda will allow you to parallel process the data at low cost and with no need to provision hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "First, lets import all the packages we will need.  Notice that all packages are native to SageMaker and we do not need to install anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import struct\n",
    "import pickle\n",
    "import botocore\n",
    "import array as arr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from struct import Struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SEGY\n",
    "We will load the SEGY file from S3 and create a StreamingBody object we can use to stream data into the notebook.  We do not need to copy the file locally.  \n",
    "\n",
    "No need to modify these variables if working in the US-East-1 region.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_bucket   = 'equinor-volve-data-village'    # S3 bucket name with input data\n",
    "source_folder   = 'Seismic/ST0202/Stacks'         # Folder path, sometimes referred to as the prefix\n",
    "source_filename = 'ST0202R08_PZ_PSDM_FULL_OFFSET_DEPTH.MIG_FIN.POST_STACK.3D.JS-017534.segy' # Filename\n",
    "\n",
    "# Get the file stream\n",
    "s3 = boto3.resource('s3')\n",
    "segy_obj = s3.Object(source_bucket, f\"{source_folder}/{source_filename}\")\n",
    "segy_stream = segy_obj.get()['Body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Headers\n",
    "Lets read the text and binary headers of the SEGY file.  We are assuming the file follows the SEGY Rev1 standard.\n",
    "\n",
    "In this format, the first 3200 bytes are the text header, the next 400 are the binary headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_header_raw = segy_stream.read(3200)\n",
    "bin_header_raw = segy_stream.read(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the text headers.  Headers are in cp500/ebcdic encoding. Here are the first 100 bytes in a raw format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_header_raw[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets decode it and make it more presentable.  It will be useful to see if there are any notes mentioning if the binary headers are stored in the correct byte locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecodeTextHeader(text_header_raw):\n",
    "    text_header = text_header_raw.decode('cp500')\n",
    "    text_header = text_header.replace(\"C 1 \", \"\\nC 1 \")\n",
    "    text_header = text_header.replace(\"C 2 \", \"\\nC 2 \")\n",
    "    text_header = text_header.replace(\"C 3 \", \"\\nC 3 \")\n",
    "    text_header = text_header.replace(\"C 4 \", \"\\nC 4 \")\n",
    "    text_header = text_header.replace(\"C 5 \", \"\\nC 5 \")\n",
    "    text_header = text_header.replace(\"C 6 \", \"\\nC 6 \")\n",
    "    text_header = text_header.replace(\"C 7 \", \"\\nC 7 \")\n",
    "    text_header = text_header.replace(\"C 8 \", \"\\nC 8 \")\n",
    "    text_header = text_header.replace(\"C 9 \", \"\\nC 9 \")\n",
    "    text_header = text_header.replace(\"C10 \", \"\\nC10 \")\n",
    "    text_header = text_header.replace(\"C11 \", \"\\nC11 \")\n",
    "    text_header = text_header.replace(\"C12 \", \"\\nC12 \")\n",
    "    text_header = text_header.replace(\"C13 \", \"\\nC13 \")\n",
    "    text_header = text_header.replace(\"C14 \", \"\\nC14 \")\n",
    "    text_header = text_header.replace(\"C15 \", \"\\nC15 \")\n",
    "    text_header = text_header.replace(\"C16 \", \"\\nC16 \")\n",
    "    text_header = text_header.replace(\"C17 \", \"\\nC17 \")\n",
    "    text_header = text_header.replace(\"C18 \", \"\\nC18 \")\n",
    "    text_header = text_header.replace(\"C19 \", \"\\nC19 \")\n",
    "    text_header = text_header.replace(\"C20 \", \"\\nC20 \")\n",
    "    text_header = text_header.replace(\"C21 \", \"\\nC21 \")\n",
    "    text_header = text_header.replace(\"C22 \", \"\\nC22 \")\n",
    "    text_header = text_header.replace(\"C23 \", \"\\nC23 \")\n",
    "    text_header = text_header.replace(\"C24 \", \"\\nC24 \")\n",
    "    text_header = text_header.replace(\"C25 \", \"\\nC25 \")\n",
    "    text_header = text_header.replace(\"C26 \", \"\\nC26 \")\n",
    "    text_header = text_header.replace(\"C27 \", \"\\nC27 \")\n",
    "    text_header = text_header.replace(\"C28 \", \"\\nC28 \")\n",
    "    text_header = text_header.replace(\"C29 \", \"\\nC29 \")\n",
    "    text_header = text_header.replace(\"C30 \", \"\\nC30 \")\n",
    "    text_header = text_header.replace(\"C31 \", \"\\nC31 \")\n",
    "    text_header = text_header.replace(\"C32 \", \"\\nC32 \")\n",
    "    text_header = text_header.replace(\"C33 \", \"\\nC33 \")\n",
    "    text_header = text_header.replace(\"C34 \", \"\\nC34 \")\n",
    "    text_header = text_header.replace(\"C35 \", \"\\nC35 \")\n",
    "    text_header = text_header.replace(\"C36 \", \"\\nC36 \")\n",
    "    text_header = text_header.replace(\"C37 \", \"\\nC37 \")\n",
    "    text_header = text_header.replace(\"C38 \", \"\\nC38 \")\n",
    "    text_header = text_header.replace(\"C39 \", \"\\nC39 \")\n",
    "    text_header = text_header.replace(\"C40 \", \"\\nC40 \")\n",
    "    \n",
    "    return text_header\n",
    "\n",
    "text_header = DecodeTextHeader(text_header_raw)\n",
    "\n",
    "print(text_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the binary header; it is raw bytes and needs to be decoded too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_header_raw[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets parse all the fields and assign them to the correct variables.  We need to convert bytes to integers, using big-endian byte order.  Most fields are 2 or 4 bytes.  This is assuming the SEGY file follows the rev1 standards correctly, which is not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for decoding the binary headers.\n",
    "def DecodeBinHeader(bin_header_raw):\n",
    "    bin_header = {}\n",
    "\n",
    "    bin_header['job_id']                  = int.from_bytes(bin_header_raw[0:4], byteorder='big', signed=False)\n",
    "    bin_header['line_no']                 = int.from_bytes(bin_header_raw[4:8], byteorder='big', signed=False)\n",
    "    bin_header['reel_no']                 = int.from_bytes(bin_header_raw[8:12], byteorder='big', signed=False)\n",
    "    bin_header['data_traces']             = int.from_bytes(bin_header_raw[12:14], byteorder='big', signed=False)\n",
    "    bin_header['aux_traces']              = int.from_bytes(bin_header_raw[14:16], byteorder='big', signed=False)\n",
    "    bin_header['sample_interval']         = int.from_bytes(bin_header_raw[16:18], byteorder='big', signed=False)\n",
    "    bin_header['sample_interval_orig']    = int.from_bytes(bin_header_raw[18:20], byteorder='big', signed=False)\n",
    "    bin_header['samples_per_trace']       = int.from_bytes(bin_header_raw[20:22], byteorder='big', signed=False)\n",
    "    bin_header['samples_per_trace_orig']  = int.from_bytes(bin_header_raw[22:24], byteorder='big', signed=False)\n",
    "    bin_header['data_sample_format']      = int.from_bytes(bin_header_raw[24:26], byteorder='big', signed=False)\n",
    "    bin_header['ensemble_fold']           = int.from_bytes(bin_header_raw[26:28], byteorder='big', signed=False)\n",
    "    bin_header['trace_sorting']           = int.from_bytes(bin_header_raw[28:30], byteorder='big', signed=False)\n",
    "    bin_header['vert_sum_code']           = int.from_bytes(bin_header_raw[30:32], byteorder='big', signed=False)\n",
    "    bin_header['sweep_hz_start']          = int.from_bytes(bin_header_raw[32:34], byteorder='big', signed=False)\n",
    "    bin_header['sweep_hz_end']            = int.from_bytes(bin_header_raw[34:36], byteorder='big', signed=False)\n",
    "    bin_header['sweep_length']            = int.from_bytes(bin_header_raw[36:38], byteorder='big', signed=False)\n",
    "    bin_header['sweep_type']              = int.from_bytes(bin_header_raw[38:40], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_ch']          = int.from_bytes(bin_header_raw[40:42], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_taper_start'] = int.from_bytes(bin_header_raw[42:44], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_taper_end']   = int.from_bytes(bin_header_raw[44:46], byteorder='big', signed=False)\n",
    "    bin_header['taper_type']              = int.from_bytes(bin_header_raw[46:48], byteorder='big', signed=False)\n",
    "    bin_header['data_traces_correlated']  = int.from_bytes(bin_header_raw[48:50], byteorder='big', signed=False)\n",
    "    bin_header['binary_gain_recovered']   = int.from_bytes(bin_header_raw[50:52], byteorder='big', signed=False)\n",
    "    bin_header['amp_recovery_method']     = int.from_bytes(bin_header_raw[52:54], byteorder='big', signed=False)\n",
    "    bin_header['measurement_system']      = int.from_bytes(bin_header_raw[54:56], byteorder='big', signed=False)\n",
    "    bin_header['impulse_sig_polarity']    = int.from_bytes(bin_header_raw[56:58], byteorder='big', signed=False)\n",
    "    bin_header['vib_polarity']            = int.from_bytes(bin_header_raw[58:60], byteorder='big', signed=False)\n",
    "    bin_header['unassigned']              = int.from_bytes(bin_header_raw[60:300], byteorder='big', signed=False)\n",
    "    bin_header['segy_format']             = int.from_bytes(bin_header_raw[300:302], byteorder='big', signed=False)\n",
    "    bin_header['fixed_length_flag']       = int.from_bytes(bin_header_raw[302:304], byteorder='big', signed=False)\n",
    "    bin_header['extended_text_header_no'] = int.from_bytes(bin_header_raw[304:306], byteorder='big', signed=False)\n",
    "    bin_header['unassigned2']             = int.from_bytes(bin_header_raw[306:400], byteorder='big', signed=False)\n",
    "    \n",
    "    return bin_header\n",
    "\n",
    "# Function for printing the binary headers.\n",
    "def PrintBinHeader(bin_header):\n",
    "    print(\"job_id                  = \", bin_header['job_id']                 )\n",
    "    print(\"line_no                 = \", bin_header['line_no']                )\n",
    "    print(\"reel_no                 = \", bin_header['reel_no']                )\n",
    "    print(\"data_traces             = \", bin_header['data_traces']            )\n",
    "    print(\"aux_traces              = \", bin_header['aux_traces']             )\n",
    "    print(\"sample_interval         = \", bin_header['sample_interval']        )\n",
    "    print(\"sample_interval_orig    = \", bin_header['sample_interval_orig']   )\n",
    "    print(\"samples_per_trace       = \", bin_header['samples_per_trace']      )\n",
    "    print(\"samples_per_trace_orig  = \", bin_header['samples_per_trace_orig'] )\n",
    "    print(\"data_sample_format      = \", bin_header['data_sample_format']     )\n",
    "    print(\"ensemble_fold           = \", bin_header['ensemble_fold']          )\n",
    "    print(\"trace_sorting           = \", bin_header['trace_sorting']          )\n",
    "    print(\"vert_sum_code           = \", bin_header['vert_sum_code']          )\n",
    "    print(\"sweep_hz_start          = \", bin_header['sweep_hz_start']         )\n",
    "    print(\"sweep_hz_end            = \", bin_header['sweep_hz_end']           )\n",
    "    print(\"sweep_length            = \", bin_header['sweep_length']           )\n",
    "    print(\"sweep_type              = \", bin_header['sweep_type']             )\n",
    "    print(\"sweep_trace_ch          = \", bin_header['sweep_trace_ch']         )\n",
    "    print(\"sweep_trace_taper_start = \", bin_header['sweep_trace_taper_start'])\n",
    "    print(\"sweep_trace_taper_end   = \", bin_header['sweep_trace_taper_end']  )\n",
    "    print(\"taper_type              = \", bin_header['taper_type']             )\n",
    "    print(\"data_traces_correlated  = \", bin_header['data_traces_correlated'] )\n",
    "    print(\"binary_gain_recovered   = \", bin_header['binary_gain_recovered']  )\n",
    "    print(\"amp_recovery_method     = \", bin_header['amp_recovery_method']    )\n",
    "    print(\"measurement_system      = \", bin_header['measurement_system']     )\n",
    "    print(\"impulse_sig_polarity    = \", bin_header['impulse_sig_polarity']   )\n",
    "    print(\"vib_polarity            = \", bin_header['vib_polarity']           )\n",
    "    print(\"unassigned              = \", bin_header['unassigned']             )\n",
    "    print(\"segy_format             = \", bin(bin_header['segy_format'])[2:]   ) # Refer to SEGY standard on how to read this \n",
    "    print(\"fixed_length_flag       = \", bin_header['fixed_length_flag']      )\n",
    "    print(\"extended_text_header_no = \", bin_header['extended_text_header_no'])\n",
    "    print(\"unassigned2             = \", bin_header['unassigned2']            )\n",
    "    \n",
    "bin_header = DecodeBinHeader(bin_header_raw)\n",
    "PrintBinHeader(bin_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traces\n",
    "After the file headers, the remainder of the SEGY is usually pairs of trace header blocks and the actual trace values in sequence.\n",
    "\n",
    "Lets read the first binary trace header and parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_header_raw = segy_stream.read(240)\n",
    "print(trace_header_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to decode it.  The function below does not read in all the data, but we are pulling what we will be needing, primarily the trace number, number of samples, and coordinates.  It is common that the trace headers do not follow the SEGY standards and need to check the text headers for the correct byte locations.  In this case, we are okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for decoding the trace headers.\n",
    "def DecodeTraceHeader(trace_header_raw):\n",
    "    trace_header = {}\n",
    "    trace_header['trace_seq_no_all']            = int.from_bytes(trace_header_raw[0:4], byteorder='big', signed=False)\n",
    "    trace_header['trace_seq_no_file']           = int.from_bytes(trace_header_raw[4:8], byteorder='big', signed=False)\n",
    "    trace_header['field_record_no_orig']        = int.from_bytes(trace_header_raw[8:12], byteorder='big', signed=False)\n",
    "    trace_header['trace_no_field_orig']         = int.from_bytes(trace_header_raw[12:16], byteorder='big', signed=False)\n",
    "    trace_header['energy_source_point_no']      = int.from_bytes(trace_header_raw[16:20], byteorder='big', signed=False)\n",
    "    trace_header['ensemble_no']                 = int.from_bytes(trace_header_raw[20:24], byteorder='big', signed=False)\n",
    "    trace_header['ensemble_trace_no']           = int.from_bytes(trace_header_raw[24:28], byteorder='big', signed=False)\n",
    "    trace_header['trace_id']                    = int.from_bytes(trace_header_raw[28:30], byteorder='big', signed=False)\n",
    "    trace_header['sum_vertical_traces']         = int.from_bytes(trace_header_raw[30:32], byteorder='big', signed=False)\n",
    "    trace_header['sum_horizontal_traces']       = int.from_bytes(trace_header_raw[32:34], byteorder='big', signed=False)\n",
    "    trace_header['data_use']                    = int.from_bytes(trace_header_raw[34:36], byteorder='big', signed=False)\n",
    "    trace_header['distance_from_source_center'] = int.from_bytes(trace_header_raw[36:40], byteorder='big', signed=False)\n",
    "    # ... incomplete\n",
    "    trace_header['group_x']                     = int.from_bytes(trace_header_raw[80:84], byteorder='big', signed=False)\n",
    "    trace_header['group_y']                     = int.from_bytes(trace_header_raw[84:88], byteorder='big', signed=False)\n",
    "    trace_header['coord_units']                 = int.from_bytes(trace_header_raw[88:90], byteorder='big', signed=False)\n",
    "    trace_header['trace_samples']               = int.from_bytes(trace_header_raw[114:116], byteorder='big', signed=False)\n",
    "\n",
    "    return trace_header\n",
    "    \n",
    "trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "\n",
    "# Function for printing the trace header variables\n",
    "def PrintTraceHeaders(trace_header):\n",
    "    print(\"trace_seq_no_all            = \", trace_header['trace_seq_no_all'])\n",
    "    print(\"trace_seq_no_file           = \", trace_header['trace_seq_no_file'])\n",
    "    print(\"field_record_no_orig        = \", trace_header['field_record_no_orig'])\n",
    "    print(\"trace_no_field_orig         = \", trace_header['trace_no_field_orig'])\n",
    "    print(\"energy_source_point_no      = \", trace_header['energy_source_point_no'])\n",
    "    print(\"ensemble_no                 = \", trace_header['ensemble_no'])\n",
    "    print(\"ensemble_trace_no           = \", trace_header['ensemble_trace_no'])\n",
    "    print(\"trace_id                    = \", trace_header['trace_id'])\n",
    "    print(\"sum_vertical_traces         = \", trace_header['sum_vertical_traces'])\n",
    "    print(\"sum_horizontal_traces       = \", trace_header['sum_horizontal_traces'])\n",
    "    print(\"data_use                    = \", trace_header['data_use'])\n",
    "    print(\"distance_from_source_center = \", trace_header['distance_from_source_center'])\n",
    "    # ... incomplete\n",
    "    print(\"group_x                     = \", trace_header['group_x'])\n",
    "    print(\"group_y                     = \", trace_header['group_y'])\n",
    "    print(\"coord_units                 = \", trace_header['coord_units'])\n",
    "    print(\"trace_samples               = \", trace_header['trace_samples'])\n",
    "\n",
    "\n",
    "PrintTraceHeaders(trace_header)\n",
    "\n",
    "# Incomplete, trace header goes until 240 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the header decoded, lets get the actual trace values that follow it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_raw = segy_stream.read(trace_header['trace_samples']*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEGY stores the trace amplitudes in different formats, outlined by the binary header \"data_sample_format\".  In this case, it is stored as \"IBM 4-byte\", which is not directly usable in Python.  Therefore we will convert it to \"IEEE 4-byte\" for easier consumption.\n",
    "\n",
    "Below is a helper function to convert the trace values. \n",
    "\n",
    "(Code Source: https://stackoverflow.com/questions/7125890/python-unpack-ibm-32-bit-float-point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that converts IBM 4-byte to IEEE\n",
    "class StructIBM32(object):\n",
    "    def __init__(self, size):\n",
    "        self.p24 = float(pow(2, 24))\n",
    "        self.unpack32int = Struct(\">%sL\" % size).unpack\n",
    "    def unpack(self, data):\n",
    "        int32 = self.unpack32int(data)\n",
    "        return [self.ibm2ieee(i) for i in int32]\n",
    "    def ibm2ieee(self, int32):\n",
    "        if int32 == 0:\n",
    "            return 0.0\n",
    "        sign = int32 >> 31 & 0x01\n",
    "        exponent = int32 >> 24 & 0x7f\n",
    "        mantissa = (int32 & 0x00ffffff) / self.p24\n",
    "        return (1 - 2 * sign) * mantissa * pow(16, exponent - 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read in the trace data from the SEGY stream and decode the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_temp = []\n",
    "converter = StructIBM32(1)\n",
    "\n",
    "for x in range(4, trace_header['trace_samples']*4+4, 4):\n",
    "    amp_int = int.from_bytes(trace_raw[x-4:x], byteorder='little', signed=False) # Convert bytes to integers\n",
    "    amp_bin = bin(amp_int)                                                       # Convert integers to binary representation\n",
    "    amplitude = converter.unpack(struct.pack('<L', int(amp_bin, 2)))             # Convert binary to actual IEEE values\n",
    "    trace_temp.append(amplitude[0])\n",
    "\n",
    "trace = np.array(trace_temp)\n",
    "print(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers are nice to see, but hard to tell if they are correctly parsed.  Lets plot the values in Matplotlib to make sure they are correct and look like a seismic wiggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = np.amax(np.absolute(trace))\n",
    "plt.figure(figsize=(5, 10))\n",
    "plt.plot(trace, range(trace_header['trace_samples']), 'red')\n",
    "plt.xlim(-limits, limits)\n",
    "plt.ylim(trace_header['trace_samples'], 0)\n",
    "plt.axvline(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks correct.  \n",
    "Lets do a simple calculation on the trace: mean amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean absolute amplitude = {:0.5f}\".format(np.mean(np.absolute(trace))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see about processing more traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_amp_mean = {}\n",
    "\n",
    "# Store the first trace result from above\n",
    "trace_amp_mean[1] = np.mean(np.absolute(trace))\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through all the remaining traces\n",
    "current_trace = 0\n",
    "while True:\n",
    "    current_trace = current_trace + 1\n",
    "\n",
    "    trace_header_raw = segy_stream.read(240)\n",
    "    trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "    trace_raw = segy_stream.read(trace_header['trace_samples']*4)\n",
    "\n",
    "    trace = []\n",
    "    converter = StructIBM32(1)\n",
    "\n",
    "    for x in range(4, trace_header['trace_samples']*4, 4):\n",
    "        amplitude = converter.unpack(struct.pack('<L', int(bin(int.from_bytes(trace_raw[x-4:x], byteorder='little', signed=False)), 2)))\n",
    "        trace.append(amplitude)\n",
    "\n",
    "    trace_amp_mean[current_trace] = np.mean(np.absolute(trace))\n",
    "\n",
    "    if current_trace%1000 == 0:\n",
    "        print(\"Trace #{} has mean absolute amplitude of {:.5f}, elapse time is {:.2f} seconds.\".format(current_trace, trace_amp_mean[current_trace], time.time() - start_time))\n",
    "\n",
    "    if current_trace > 5000:\n",
    "        print(\"Stopping here, this is taking longer than needed.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is too slow.  Lets clean up the S3 stream and try something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segy_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Out\n",
    "\n",
    "Processing the SEGY file with a single machine is quite slow.  It would take about 18 minutes to complete on this T2.Medium instance.  We could scale up and use a more powerful instance type, but then we are paying per hour, even if we are not using the full compute capacity.  It can quickly start becoming expensive to leave the machines running.  Plus this is not taking advantage of cloud services.\n",
    "\n",
    "So lets leverage the power of the cloud to scale out and distribute the processing workload with on-demand processing, Lambda!  Lets split this workload and send it to 100 Lambdas that can run in parallel.  We do not need to send any seismic data to Lambda, we will simply tell it where the file is and which bytes to load.\n",
    "\n",
    "Put in the bucket name you created when deploying the CloudFormation template.\n",
    "\n",
    "There is a Lambda function that contains the same calculations we performed above called [SegyBatchProcessMeanAmp](https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/SegyBatchProcessMeanAmp).  If you want, go check it out in the AWS Console.  It was deployed with the CF template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bucket     = \"vavourak-demo-temp\"                  # Bucket to use, created in the CloudFormation template\n",
    "mean_amp_folder    = \"temp-trace-bundles-ST0202-mean-amp\"  # Subfolder to place the calculation results\n",
    "lambda_name        = \"SegyBatchProcessMeanAmp\"             # Name of the Lambda function to invoke\n",
    "concurrent_lambdas = 100                                   # Number of Lambdas to invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get S3 object\n",
    "segy_obj = s3.Object(source_bucket, f\"{source_folder}/{source_filename}\")\n",
    "\n",
    "# Define some needed variables based off the above parameters\n",
    "start_time = time.time()\n",
    "header_size = 3600\n",
    "trace_header_size = 240\n",
    "trace_size = bin_header['samples_per_trace'] * 4\n",
    "trace_size_with_headers = trace_size + trace_header_size\n",
    "filesize = segy_obj.content_length\n",
    "trace_count = int((filesize - 3600) / trace_size_with_headers)\n",
    "bundle_size = round(trace_count/concurrent_lambdas)\n",
    "\n",
    "results_file_list = [] # Lets keep track of the output file names, so we can grab them later\n",
    "\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "print(f\"Total traces in file: {trace_count}\")\n",
    "print(f\"Traces per Lambda for {concurrent_lambdas} concurrency (not rounded): {trace_count/concurrent_lambdas}\")\n",
    "print(f\"Traces per Lambda, rounded up: {round(trace_count/concurrent_lambdas+0.5)}\")\n",
    "\n",
    "# Send the trace bundle information over to Lambda\n",
    "for bundle in range(0, int(trace_count), bundle_size):\n",
    "    bytes_start = bundle * trace_size_with_headers + header_size\n",
    "    bytes_stop = (bundle + bundle_size) * trace_size_with_headers + header_size - 1\n",
    "    #print(f\"Bundle traces: {bundle}-{bundle+bundle_size}, bytes: {bytes_start}-{bytes_stop}.\")\n",
    "    \n",
    "    # Build the message for the Lambda to find the seismic file\n",
    "    payload = {\n",
    "        \"bucket_in\"          : source_bucket,\n",
    "        \"folder_in\"          : source_folder,\n",
    "        \"filename_in\"        : source_filename,\n",
    "        \"bucket_out\"         : results_bucket,\n",
    "        \"folder_out\"         : mean_amp_folder,\n",
    "        \"bytes_start\"        : bytes_start,\n",
    "        \"bytes_stop\"         : bytes_stop,\n",
    "        \"use_custom_lines\"   : 0,\n",
    "        \"data_sample_format\" : bin_header['data_sample_format']\n",
    "    }\n",
    "\n",
    "    # Invoke the Lambda SegyBatchProcessMeanAmp\n",
    "    response = lambda_client.invoke(FunctionName=lambda_name,\n",
    "                                    InvocationType='Event',\n",
    "                                    Payload=json.dumps(payload))\n",
    "    \n",
    "    results_file_list.append(f\"{mean_amp_folder}/{source_filename}.{bytes_start}-{bytes_stop}.pkl\")\n",
    "\n",
    "print(\"Done!  Elapse time to gather trace info and send to Lambda: {:0.2f} seconds.  Now waiting for 80 seconds.\".format(time.time() - start_time))\n",
    "\n",
    "time.sleep(80)      # Waiting before carrying on next steps, to allow time for Lambda to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The trace bundles are all being processed in parallel by Lambda.  It should take about 1 minute to complete instead of 18!  With Lambda, we do not need to provision any resources or have servers waiting with spare compute capacity.  It is all managed in the background by AWS and you only pay for the compute time needed to run the code.  The cost to do the calculation on this 800MB SEGY file is only $0.02.\n",
    "\n",
    "The Lambdas should be done by now.  Lets load in the results from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "trace_mean_amp = []\n",
    "trace_x = []\n",
    "trace_y = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through the files\n",
    "for x in range(0, len(results_file_list)):\n",
    "    print(\"Reading file: \", results_file_list[x])\n",
    "    \n",
    "    # Get file from S3, convert from Pickle format\n",
    "    object = s3_client.get_object(Bucket=results_bucket, Key=results_file_list[x])\n",
    "    serializedObject = object['Body'].read()\n",
    "    trace_bundle_temp = pickle.loads(serializedObject)\n",
    "    \n",
    "    # Split the tuple [1,2,3] into seperate variables for easier use\n",
    "    for y in range(0, len(trace_bundle_temp)):\n",
    "        trace_mean_amp.append(trace_bundle_temp[y][0])\n",
    "        trace_x.append(trace_bundle_temp[y][1])\n",
    "        trace_y.append(trace_bundle_temp[y][2])\n",
    "\n",
    "print(\"Number of traces loaded: {}, elapsed time: {:0.2f} seconds.\".format(len(trace_mean_amp), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets map out the mean amplitudes using a Matplotlib scatter plot.  The Lambda saved the mean amplitude, along with the X and Y coordinates of each trace group for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(trace_x, trace_y, c=trace_mean_amp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!  Seems like there is some banding along the acquisition lines.  Maybe in the future we can make a Lambda function to balance out the variations along each inline/xline.\n",
    "\n",
    "\n",
    "### Clean-up\n",
    "Lets clean up the files we created.  We could keep them and catalog them in DynamoDB for future use in a production environment so we do not need rerun calculations, but for today we no longer need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(results_file_list)):\n",
    "    _ = s3_client.delete_object(Bucket=results_bucket, Key=results_file_list[x])\n",
    "    print(\"Deleting: {}\".format(results_file_list[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "Lets scale this project out by 2 orders of magnitude!  Instead of processing only 800MB, which is no real feat of engineering and can be done in-memory even on a cell phone these days, we will process a massive 180GB pre-stack raw seismic file.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
