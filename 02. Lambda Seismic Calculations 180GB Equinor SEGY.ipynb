{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Scale Distributed Seismic Processing\n",
    "\n",
    "In this notebook we will work with a much larger dataset to push the limits a bit.  Here we have a 180GB SEGY file containing raw pre-stack seismic data sitting in an S3 bucket.  Lets take a look at this dataset and then do the same mean amplitude calculations, using only a basic T2.Medium EC2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import struct\n",
    "import pickle\n",
    "import botocore\n",
    "import array as arr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from struct import Struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "We will reuse the SEGY loading functions from before, so lets define them all in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecodeTextHeader(text_header_raw):\n",
    "    text_header = text_header_raw.decode('cp500')\n",
    "    text_header = text_header.replace(\"C 1 \", \"\\nC 1 \")\n",
    "    text_header = text_header.replace(\"C 2 \", \"\\nC 2 \")\n",
    "    text_header = text_header.replace(\"C 3 \", \"\\nC 3 \")\n",
    "    text_header = text_header.replace(\"C 4 \", \"\\nC 4 \")\n",
    "    text_header = text_header.replace(\"C 5 \", \"\\nC 5 \")\n",
    "    text_header = text_header.replace(\"C 6 \", \"\\nC 6 \")\n",
    "    text_header = text_header.replace(\"C 7 \", \"\\nC 7 \")\n",
    "    text_header = text_header.replace(\"C 8 \", \"\\nC 8 \")\n",
    "    text_header = text_header.replace(\"C 9 \", \"\\nC 9 \")\n",
    "    text_header = text_header.replace(\"C10 \", \"\\nC10 \")\n",
    "    text_header = text_header.replace(\"C11 \", \"\\nC11 \")\n",
    "    text_header = text_header.replace(\"C12 \", \"\\nC12 \")\n",
    "    text_header = text_header.replace(\"C13 \", \"\\nC13 \")\n",
    "    text_header = text_header.replace(\"C14 \", \"\\nC14 \")\n",
    "    text_header = text_header.replace(\"C15 \", \"\\nC15 \")\n",
    "    text_header = text_header.replace(\"C16 \", \"\\nC16 \")\n",
    "    text_header = text_header.replace(\"C17 \", \"\\nC17 \")\n",
    "    text_header = text_header.replace(\"C18 \", \"\\nC18 \")\n",
    "    text_header = text_header.replace(\"C19 \", \"\\nC19 \")\n",
    "    text_header = text_header.replace(\"C20 \", \"\\nC20 \")\n",
    "    text_header = text_header.replace(\"C21 \", \"\\nC21 \")\n",
    "    text_header = text_header.replace(\"C22 \", \"\\nC22 \")\n",
    "    text_header = text_header.replace(\"C23 \", \"\\nC23 \")\n",
    "    text_header = text_header.replace(\"C24 \", \"\\nC24 \")\n",
    "    text_header = text_header.replace(\"C25 \", \"\\nC25 \")\n",
    "    text_header = text_header.replace(\"C26 \", \"\\nC26 \")\n",
    "    text_header = text_header.replace(\"C27 \", \"\\nC27 \")\n",
    "    text_header = text_header.replace(\"C28 \", \"\\nC28 \")\n",
    "    text_header = text_header.replace(\"C29 \", \"\\nC29 \")\n",
    "    text_header = text_header.replace(\"C30 \", \"\\nC30 \")\n",
    "    text_header = text_header.replace(\"C31 \", \"\\nC31 \")\n",
    "    text_header = text_header.replace(\"C32 \", \"\\nC32 \")\n",
    "    text_header = text_header.replace(\"C33 \", \"\\nC33 \")\n",
    "    text_header = text_header.replace(\"C34 \", \"\\nC34 \")\n",
    "    text_header = text_header.replace(\"C35 \", \"\\nC35 \")\n",
    "    text_header = text_header.replace(\"C36 \", \"\\nC36 \")\n",
    "    text_header = text_header.replace(\"C37 \", \"\\nC37 \")\n",
    "    text_header = text_header.replace(\"C38 \", \"\\nC38 \")\n",
    "    text_header = text_header.replace(\"C39 \", \"\\nC39 \")\n",
    "    text_header = text_header.replace(\"C40 \", \"\\nC40 \")\n",
    "    \n",
    "    return text_header\n",
    "\n",
    "\n",
    "def DecodeBinHeader(bin_header_raw):\n",
    "    bin_header = {}\n",
    "\n",
    "    bin_header['job_id']                  = int.from_bytes(bin_header_raw[0:4], byteorder='big', signed=False)\n",
    "    bin_header['line_no']                 = int.from_bytes(bin_header_raw[4:8], byteorder='big', signed=False)\n",
    "    bin_header['reel_no']                 = int.from_bytes(bin_header_raw[8:12], byteorder='big', signed=False)\n",
    "    bin_header['data_traces']             = int.from_bytes(bin_header_raw[12:14], byteorder='big', signed=False)\n",
    "    bin_header['aux_traces']              = int.from_bytes(bin_header_raw[14:16], byteorder='big', signed=False)\n",
    "    bin_header['sample_interval']         = int.from_bytes(bin_header_raw[16:18], byteorder='big', signed=False)\n",
    "    bin_header['sample_interval_orig']    = int.from_bytes(bin_header_raw[18:20], byteorder='big', signed=False)\n",
    "    bin_header['samples_per_trace']       = int.from_bytes(bin_header_raw[20:22], byteorder='big', signed=False)\n",
    "    bin_header['samples_per_trace_orig']  = int.from_bytes(bin_header_raw[22:24], byteorder='big', signed=False)\n",
    "    bin_header['data_sample_format']      = int.from_bytes(bin_header_raw[24:26], byteorder='big', signed=False)\n",
    "    bin_header['ensemble_fold']           = int.from_bytes(bin_header_raw[26:28], byteorder='big', signed=False)\n",
    "    bin_header['trace_sorting']           = int.from_bytes(bin_header_raw[28:30], byteorder='big', signed=False)\n",
    "    bin_header['vert_sum_code']           = int.from_bytes(bin_header_raw[30:32], byteorder='big', signed=False)\n",
    "    bin_header['sweep_hz_start']          = int.from_bytes(bin_header_raw[32:34], byteorder='big', signed=False)\n",
    "    bin_header['sweep_hz_end']            = int.from_bytes(bin_header_raw[34:36], byteorder='big', signed=False)\n",
    "    bin_header['sweep_length']            = int.from_bytes(bin_header_raw[36:38], byteorder='big', signed=False)\n",
    "    bin_header['sweep_type']              = int.from_bytes(bin_header_raw[38:40], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_ch']          = int.from_bytes(bin_header_raw[40:42], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_taper_start'] = int.from_bytes(bin_header_raw[42:44], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_taper_end']   = int.from_bytes(bin_header_raw[44:46], byteorder='big', signed=False)\n",
    "    bin_header['taper_type']              = int.from_bytes(bin_header_raw[46:48], byteorder='big', signed=False)\n",
    "    bin_header['data_traces_correlated']  = int.from_bytes(bin_header_raw[48:50], byteorder='big', signed=False)\n",
    "    bin_header['binary_gain_recovered']   = int.from_bytes(bin_header_raw[50:52], byteorder='big', signed=False)\n",
    "    bin_header['amp_recovery_method']     = int.from_bytes(bin_header_raw[52:54], byteorder='big', signed=False)\n",
    "    bin_header['measurement_system']      = int.from_bytes(bin_header_raw[54:56], byteorder='big', signed=False)\n",
    "    bin_header['impulse_sig_polarity']    = int.from_bytes(bin_header_raw[56:58], byteorder='big', signed=False)\n",
    "    bin_header['vib_polarity']            = int.from_bytes(bin_header_raw[58:60], byteorder='big', signed=False)\n",
    "    bin_header['unassigned']              = int.from_bytes(bin_header_raw[60:300], byteorder='big', signed=False)\n",
    "    bin_header['segy_format']             = int.from_bytes(bin_header_raw[300:302], byteorder='big', signed=False)\n",
    "    bin_header['fixed_length_flag']       = int.from_bytes(bin_header_raw[302:304], byteorder='big', signed=False)\n",
    "    bin_header['extended_text_header_no'] = int.from_bytes(bin_header_raw[304:306], byteorder='big', signed=False)\n",
    "    bin_header['unassigned2']             = int.from_bytes(bin_header_raw[306:400], byteorder='big', signed=False)\n",
    "    \n",
    "    return bin_header\n",
    "\n",
    "\n",
    "def PrintBinHeader(bin_header):\n",
    "    print(\"job_id                  = \", bin_header['job_id']                 )\n",
    "    print(\"line_no                 = \", bin_header['line_no']                )\n",
    "    print(\"reel_no                 = \", bin_header['reel_no']                )\n",
    "    print(\"data_traces             = \", bin_header['data_traces']            )\n",
    "    print(\"aux_traces              = \", bin_header['aux_traces']             )\n",
    "    print(\"sample_interval         = \", bin_header['sample_interval']        )\n",
    "    print(\"sample_interval_orig    = \", bin_header['sample_interval_orig']   )\n",
    "    print(\"samples_per_trace       = \", bin_header['samples_per_trace']      )\n",
    "    print(\"samples_per_trace_orig  = \", bin_header['samples_per_trace_orig'] )\n",
    "    print(\"data_sample_format      = \", bin_header['data_sample_format']     )\n",
    "    print(\"ensemble_fold           = \", bin_header['ensemble_fold']          )\n",
    "    print(\"trace_sorting           = \", bin_header['trace_sorting']          )\n",
    "    print(\"vert_sum_code           = \", bin_header['vert_sum_code']          )\n",
    "    print(\"sweep_hz_start          = \", bin_header['sweep_hz_start']         )\n",
    "    print(\"sweep_hz_end            = \", bin_header['sweep_hz_end']           )\n",
    "    print(\"sweep_length            = \", bin_header['sweep_length']           )\n",
    "    print(\"sweep_type              = \", bin_header['sweep_type']             )\n",
    "    print(\"sweep_trace_ch          = \", bin_header['sweep_trace_ch']         )\n",
    "    print(\"sweep_trace_taper_start = \", bin_header['sweep_trace_taper_start'])\n",
    "    print(\"sweep_trace_taper_end   = \", bin_header['sweep_trace_taper_end']  )\n",
    "    print(\"taper_type              = \", bin_header['taper_type']             )\n",
    "    print(\"data_traces_correlated  = \", bin_header['data_traces_correlated'] )\n",
    "    print(\"binary_gain_recovered   = \", bin_header['binary_gain_recovered']  )\n",
    "    print(\"amp_recovery_method     = \", bin_header['amp_recovery_method']    )\n",
    "    print(\"measurement_system      = \", bin_header['measurement_system']     )\n",
    "    print(\"impulse_sig_polarity    = \", bin_header['impulse_sig_polarity']   )\n",
    "    print(\"vib_polarity            = \", bin_header['vib_polarity']           )\n",
    "    print(\"unassigned              = \", bin_header['unassigned']             )\n",
    "    print(\"segy_format             = \", bin(bin_header['segy_format'])[2:]   )    \n",
    "    print(\"fixed_length_flag       = \", bin_header['fixed_length_flag']      )\n",
    "    print(\"extended_text_header_no = \", bin_header['extended_text_header_no'])\n",
    "    print(\"unassigned2             = \", bin_header['unassigned2']            )\n",
    "    \n",
    "    \n",
    "def DecodeTraceHeader(trace_header_raw):\n",
    "    trace_header = {}\n",
    "    trace_header['trace_seq_no_all']            = int.from_bytes(trace_header_raw[0:4], byteorder='big', signed=False)\n",
    "    trace_header['trace_seq_no_file']           = int.from_bytes(trace_header_raw[4:8], byteorder='big', signed=False)\n",
    "    trace_header['field_record_no_orig']        = int.from_bytes(trace_header_raw[8:12], byteorder='big', signed=False)\n",
    "    trace_header['trace_no_field_orig']         = int.from_bytes(trace_header_raw[12:16], byteorder='big', signed=False)\n",
    "    trace_header['energy_source_point_no']      = int.from_bytes(trace_header_raw[16:20], byteorder='big', signed=False)\n",
    "    trace_header['ensemble_no']                 = int.from_bytes(trace_header_raw[20:24], byteorder='big', signed=False)\n",
    "    trace_header['ensemble_trace_no']           = int.from_bytes(trace_header_raw[24:28], byteorder='big', signed=False)\n",
    "    trace_header['trace_id']                    = int.from_bytes(trace_header_raw[28:30], byteorder='big', signed=False)\n",
    "    trace_header['sum_vertical_traces']         = int.from_bytes(trace_header_raw[30:32], byteorder='big', signed=False)\n",
    "    trace_header['sum_horizontal_traces']       = int.from_bytes(trace_header_raw[32:34], byteorder='big', signed=False)\n",
    "    trace_header['data_use']                    = int.from_bytes(trace_header_raw[34:36], byteorder='big', signed=False)\n",
    "    trace_header['distance_from_source_center'] = int.from_bytes(trace_header_raw[36:40], byteorder='big', signed=False)\n",
    "    # ... incomplete\n",
    "    trace_header['group_x']                     = int.from_bytes(trace_header_raw[80:84], byteorder='big', signed=False)\n",
    "    trace_header['group_y']                     = int.from_bytes(trace_header_raw[84:88], byteorder='big', signed=False)\n",
    "    trace_header['coord_units']                 = int.from_bytes(trace_header_raw[88:90], byteorder='big', signed=False)\n",
    "    trace_header['trace_samples']               = int.from_bytes(trace_header_raw[114:116], byteorder='big', signed=False)\n",
    "    trace_header['sample_interval']             = int.from_bytes(trace_header_raw[116:118], byteorder='big', signed=False)\n",
    "    trace_header['gain_type']                   = int.from_bytes(trace_header_raw[118:120], byteorder='big', signed=False)\n",
    "    trace_header['CDP_X']                       = int.from_bytes(trace_header_raw[180:184], byteorder='big', signed=False)\n",
    "    trace_header['CDP_Y']                       = int.from_bytes(trace_header_raw[184:188], byteorder='big', signed=False)\n",
    "    trace_header['inline']                      = int.from_bytes(trace_header_raw[188:192], byteorder='big', signed=False)\n",
    "    trace_header['xline']                       = int.from_bytes(trace_header_raw[192:196], byteorder='big', signed=False)\n",
    "    trace_header['trace_unit']                  = int.from_bytes(trace_header_raw[202:204], byteorder='big', signed=False) \n",
    "    trace_header['inline_custom']               = int.from_bytes(trace_header_raw[232:236], byteorder='big', signed=False)\n",
    "    trace_header['xline_custom']                = int.from_bytes(trace_header_raw[236:240], byteorder='big', signed=False)\n",
    "\n",
    "    return trace_header\n",
    "    \n",
    "\n",
    "def PrintTraceHeaders(trace_header):\n",
    "    print(\"trace_seq_no_all            = \", trace_header['trace_seq_no_all'])\n",
    "    print(\"trace_seq_no_file           = \", trace_header['trace_seq_no_file'])\n",
    "    print(\"field_record_no_orig        = \", trace_header['field_record_no_orig'])\n",
    "    print(\"trace_no_field_orig         = \", trace_header['trace_no_field_orig'])\n",
    "    print(\"energy_source_point_no      = \", trace_header['energy_source_point_no'])\n",
    "    print(\"ensemble_no                 = \", trace_header['ensemble_no'])\n",
    "    print(\"ensemble_trace_no           = \", trace_header['ensemble_trace_no'])\n",
    "    print(\"trace_id                    = \", trace_header['trace_id'])\n",
    "    print(\"sum_vertical_traces         = \", trace_header['sum_vertical_traces'])\n",
    "    print(\"sum_horizontal_traces       = \", trace_header['sum_horizontal_traces'])\n",
    "    print(\"data_use                    = \", trace_header['data_use'])\n",
    "    print(\"distance_from_source_center = \", trace_header['distance_from_source_center'])\n",
    "    # ... incomplete\n",
    "    print(\"group_x                     = \", trace_header['group_x'])\n",
    "    print(\"group_y                     = \", trace_header['group_y'])\n",
    "    print(\"coord_units                 = \", trace_header['coord_units'])\n",
    "    print(\"trace_samples               = \", trace_header['trace_samples'])\n",
    "    print(\"sample_interval             = \", trace_header['sample_interval'])\n",
    "    print(\"gain_type                   = \", trace_header['gain_type'])\n",
    "    print(\"CDP_X                       = \", trace_header['CDP_X'])          \n",
    "    print(\"CDP_Y                       = \", trace_header['CDP_Y'])          \n",
    "    print(\"inline                      = \", trace_header['inline'])         \n",
    "    print(\"xline                       = \", trace_header['xline'])          \n",
    "    print(\"trace_unit                  = \", trace_header['trace_unit'])     \n",
    "    print(\"inline_custom               = \", trace_header['inline_custom'])         \n",
    "    print(\"xline_custom                = \", trace_header['xline_custom'])          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SEGY File\n",
    "\n",
    "Lets load in the large 180GB SEGY file as a data stream.  Again, when you attempt this, adjust the variables to point towards your own seismic files residing in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the 180GB raw pre-stack SEGY data\n",
    "\n",
    "source_bucket   = 'equinor-volve-data-village'                  # S3 bucket name with input data\n",
    "source_folder   = 'Seismic/ST10010/Raw_data/ST10010+NAV_MERGE'  # Folder path\n",
    "source_filename = 'ST10010_1150780_40203.sgy'                   # Filename\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "segy_obj = s3.Object(source_bucket, f\"{source_folder}/{source_filename}\")\n",
    "segy_stream = segy_obj.get()['Body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Headers\n",
    "\n",
    "Lets read the text and binary headers.  We will see that this file does not follow SEGY revision 1 standard as before and has custom trace header byte locations for some information.  We will not be using that data for this notebook.  If we did, we would need to adapt our function above to query the correct locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_header_raw = segy_stream.read(3200)\n",
    "bin_header_raw = segy_stream.read(400)\n",
    "\n",
    "text_header = DecodeTextHeader(text_header_raw)\n",
    "\n",
    "print(\"Text Header:\")\n",
    "print(text_header)\n",
    "\n",
    "bin_header = DecodeBinHeader(bin_header_raw)\n",
    "print(\"\\nBinary Header:\")\n",
    "PrintBinHeader(bin_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace Headers\n",
    "\n",
    "Reading the first trace header.  Notice CDP X/Y and inline/xline values are not correct, as they are stored at another byte location, as per the text header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_header_raw = segy_stream.read(240)\n",
    "\n",
    "trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "PrintTraceHeaders(trace_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace Data\n",
    "\n",
    "Lets read in the first trace and plot it.  In this file the trace data is in IEEE 4-byte float, which is useable in Python.  We do not need to convert it, but only unpack it from the byte format.\n",
    "\n",
    "This raw data looks very different than the processed data we saw before, as it does not have any amplitude gain applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_raw = segy_stream.read(trace_header['trace_samples']*4)\n",
    "\n",
    "trace = []\n",
    "for x in range(4, trace_header['trace_samples']*4+4, 4):\n",
    "    trace.append(struct.unpack('>f', trace_raw[x-4:x])[0])\n",
    "\n",
    "# Lets plot the trace\n",
    "limits = np.amax(np.absolute(trace))\n",
    "plt.figure(figsize=(5, 10))\n",
    "plt.plot(trace, range(trace_header['trace_samples']), 'red')\n",
    "plt.xlim(-limits, limits)\n",
    "plt.ylim(trace_header['trace_samples'], 0)\n",
    "plt.axvline(0, c='grey')\n",
    "\n",
    "\n",
    "segy_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data\n",
    "\n",
    "Great, we are loading the data correctly.  Now to scale this out across multiple Lambdas.\n",
    "\n",
    "First, for a benchmark, lets start processing here and see how long it would take on a single vCPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segy_stream = segy_obj.get()['Body']\n",
    "\n",
    "text_header_raw = segy_stream.read(3200)\n",
    "bin_header_raw = segy_stream.read(400)\n",
    "text_header = DecodeTextHeader(text_header_raw)\n",
    "bin_header = DecodeBinHeader(bin_header_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_amp_mean = {}\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through all the traces\n",
    "while True:\n",
    "    trace_header_raw = segy_stream.read(240)\n",
    "    trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "    trace_raw = segy_stream.read(trace_header['trace_samples']*4)\n",
    "\n",
    "    trace = []\n",
    "    for x in range(4, trace_header['trace_samples']*4+4, 4):\n",
    "        trace.append(struct.unpack('>f', trace_raw[x-4:x])[0])\n",
    "    \n",
    "    trace_amp_mean[trace_header['trace_seq_no_all']] = np.mean(np.absolute(trace))\n",
    "    \n",
    "    if trace_header['trace_seq_no_all']%1000 == 0:\n",
    "        print(\"Trace #{} has a mean amplitude of {}, elapse time is {:.2f} seconds.\".format(\n",
    "                                        trace_header['trace_seq_no_all'], \n",
    "                                        trace_amp_mean[trace_header['trace_seq_no_all']], \n",
    "                                        time.time() - start_time))\n",
    "        \n",
    "    if trace_header['trace_seq_no_all'] > 5000:\n",
    "        print(\"Stopping here.\")\n",
    "        break\n",
    "\n",
    "segy_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Out\n",
    "\n",
    "So thats about 2-3 seconds per 1,000 traces.  Problem is that there are about 9.3 million traces in this file.  That would take about 6 hours.  Plus the entire raw dataset for the seismic cube contains many 180GB files.\n",
    "\n",
    "We could scale up and use a 16xLarge instance (64 CPUs) and start threading the calculations.  This might take about 6 mins to process, but now we will have file bandwidth bottlenecks trying to read in the file into the EC2 instance.  It takes about 75 minutes to read through the file from S3 to this notebook.  A further downside is that you have to do the undiferentiated heavy lifting of provisioning the hardware resources for the notebook and starting/stopping them as needed to save costs.  The T3.16xLarge instance cost is about \\\\$3/hour, versus $0.03/hour for our T2.Medium.  Also, once your testing is done, all this code can be moved to a Lambda function to be call only as-needed.\n",
    "\n",
    "So lets use Lambda again to process the file.  No need for provisioning resources and take advantage of S3's file transfer bandwidth.  With our current default limits, we can have 1000 concurrent Lambdas running, so we will use them all!  This limit can be increased if needed (and should, if doing seismic calculations like this).\n",
    "\n",
    "Lets get things setup up below.  Adjust variables below to an S3 bucket you have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_name        = \"SegyBatchProcessMeanAmp\"             # Name of the Lambda function to invoke\n",
    "results_bucket     = \"vavourak-testing\"                    # Bucket to use\n",
    "mean_amp_folder    = \"temp-trace-bundles-st10010-mean-amp\" # Subfolder to place the calculation results\n",
    "concurrent_lambdas = 1000                                  # Number of Lambdas to invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split up the SEGY file into byte ranges and invoke 1000 Lambda functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get S3 object\n",
    "segy_obj = s3.Object(source_bucket, f\"{source_folder}/{source_filename}\")\n",
    "\n",
    "# Define some needed variables based off the above parameters\n",
    "start_time = time.time()\n",
    "header_size = 3600\n",
    "trace_header_size = 240\n",
    "trace_size = bin_header['samples_per_trace'] * 4\n",
    "trace_size_with_headers = trace_size + trace_header_size\n",
    "filesize = segy_obj.content_length\n",
    "trace_count = int((filesize - 3600) / trace_size_with_headers)\n",
    "bundle_size = round(trace_count/concurrent_lambdas)\n",
    "lambda_counter = 0\n",
    "\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "results_file_list = [] # Lets keep track of the output file names, so we can grab them later\n",
    "\n",
    "print(f\"Total traces in file: {trace_count}\")\n",
    "print(f\"Traces per Lambda for {concurrent_lambdas} concurrency (not rounded): {trace_count/concurrent_lambdas}\")\n",
    "print(f\"Traces per Lambda, rounded up: {round(trace_count/concurrent_lambdas+0.5)}\")\n",
    "\n",
    "# Send the trace bundle information over to Lambda\n",
    "for bundle in range(0, int(trace_count), bundle_size):\n",
    "    lambda_counter = lambda_counter + 1\n",
    "    bytes_start = bundle * trace_size_with_headers + header_size\n",
    "    bytes_stop = (bundle + bundle_size) * trace_size_with_headers + header_size - 1\n",
    "    print(f\"Lambda #{lambda_counter}, bundled traces: {bundle}-{bundle+bundle_size}, bytes: {bytes_start}-{bytes_stop}.\")\n",
    "    \n",
    "    # Build the message for the Lambda to find the files\n",
    "    payload = {\n",
    "        \"bucket_in\"          : source_bucket,\n",
    "        \"folder_in\"          : source_folder,\n",
    "        \"filename_in\"        : source_filename,\n",
    "        \"bucket_out\"         : results_bucket,\n",
    "        \"folder_out\"         : mean_amp_folder,\n",
    "        \"bytes_start\"        : bytes_start,\n",
    "        \"bytes_stop\"         : bytes_stop,\n",
    "        \"use_custom_lines\"   : 1,\n",
    "        \"data_sample_format\" : bin_header['data_sample_format']\n",
    "    }\n",
    "\n",
    "    # Invoke the Lambda SegyBatchProcessMeanAmp\n",
    "    response = lambda_client.invoke(FunctionName=lambda_name,\n",
    "                                    InvocationType='Event',\n",
    "                                    Payload=json.dumps(payload))\n",
    "\n",
    "    results_file_list.append(f\"{mean_amp_folder}/{source_filename}.{bytes_start}-{bytes_stop}.pkl\")\n",
    "    \n",
    "print(\"Done!  Elapse time to gather traces and send to Lambda: {:0.2f} seconds, now waiting a bit for processing to complete.\".format(time.time() - start_time))\n",
    "\n",
    "time.sleep(400)      # Waiting before carrying on next steps, to allow time for Lambda to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a 1000 Lambdas, taking roughly 330 seconds each, 60 seconds to start them, our total time is about 6.5 minutes.  The cost to perform this calculation on this 180GB file is about $0.67.  As it takes 60 seconds to invoke the Lambdas, there is some room for optimization here to invoke them faster.  Having more Lambda concurrency available will help further, as each one can process a smaller portion of the file.  \n",
    "\n",
    "Our current architecture:\n",
    "\n",
    "\n",
    "![title](images/Page-2.png)\n",
    "\n",
    "\n",
    "As mentioned before, leveraging Step Functions would be beneficial here to monitor the status of the Lambdas and notify us once they are all complete to trigger the next steps automatically.  Furthermore, using EventBridge, the Step Function can automatically run when a new seismic file gets uploaded to the S3 bucket or if there is an external trigger:\n",
    "\n",
    "\n",
    "![title](images/Page-3.png)\n",
    "\n",
    "\n",
    "### Load Results\n",
    "The Lambdas should be done by now. Lets load in the results from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "traces = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through the files\n",
    "for x in range(0, len(results_file_list)):\n",
    "    print(\"Reading file: \", results_file_list[x])\n",
    "    \n",
    "    # Get file from S3, convert from Pickle format\n",
    "    object = s3_client.get_object(Bucket=results_bucket, Key=results_file_list[x])\n",
    "    serializedObject = object['Body'].read()\n",
    "    trace_bundle_temp = pickle.loads(serializedObject)\n",
    "    \n",
    "    for y in range(0, len(trace_bundle_temp)):\n",
    "        traces.append(trace_bundle_temp[y])\n",
    "\n",
    "print(\"Elapsed time: {:0.2f} seconds.\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting over 9 million traces will be taxing for Matplotlib.  Lets only show the max mean amplitude for each inline-xline location.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_grouped = {}\n",
    "traces_max = {}\n",
    "\n",
    "# Go through every trace and make a dictionary with the values at each unique inline/xline location\n",
    "counter = 0\n",
    "for trace in traces:\n",
    "    counter = counter + 1\n",
    "    group = f\"{trace[1]}-{trace[2]}\"\n",
    "    \n",
    "    if group in traces_grouped:\n",
    "        traces_grouped[str(group)].append(trace[0])\n",
    "    else:\n",
    "        traces_grouped[str(group)] = [trace[0]]\n",
    "\n",
    "# Find the max amplitude at each location\n",
    "for group in traces_grouped:\n",
    "    traces_max[group] = np.amax(traces_grouped[group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary back to arrays for easy consuption\n",
    "traces_max_value = []\n",
    "traces_max_inline = []\n",
    "traces_max_xline = []\n",
    "\n",
    "counter = 0\n",
    "for group in traces_max:\n",
    "    inline = int(group.split(\"-\")[0])\n",
    "    xline = int(group.split(\"-\")[1])\n",
    "    \n",
    "    traces_max_inline.append(inline)\n",
    "    traces_max_xline.append(xline)\n",
    "    traces_max_value.append(traces_max[group])\n",
    "    counter = counter + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets map out the mean amplitudes using a Matplotlib scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 10))\n",
    "plt.scatter(traces_max_inline, traces_max_xline, c=traces_max_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We have successfully performed a mathematical calculation on a 180GB seismic SEGY file without having to spin up or down a single computer.  All at the cost of $0.67 and 6.5 minutes of processing time.  This process is scalable and repeatable.  It can be expanded upon with AWS Step Functions to orchestrate multiple steps and SageMaker to perform ML inference on the data.  Lambda concurrency can even be increased from the default 1000 to hundreds of thousands, greatly increasing performance.  \n",
    "\n",
    "Of course, this calculation is not at the complexity of Full Waveform Inversion or other computations that require high performance computing (HPC).  However, if those calculations can be adjusted to work with Lambda (up to 6 vCPU, 10GB RAM, 15 minute limit), a huge amount of undifferentiated heavy lifting can be removed and offer an alternative to the complixity of scheduling thousands of EC2 spot instances.  Cost differences would have to be explored too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Lets clean up the files we generated, so that we can delete the CloudFormation stack without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(results_file_list)):\n",
    "    _ = s3_client.delete_object(Bucket=results_bucket, Key=results_file_list[x])\n",
    "    print(\"Deleting: {}\".format(results_file_list[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
