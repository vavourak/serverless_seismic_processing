{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Scale Distributed Seismic Processing - Poseidon Dataset\n",
    "\n",
    "This is a workflow where we will work through performing calculations on post-stack seismic data using cloud computing.\n",
    "\n",
    "We will use this Jupyter notebook to walk through the steps, S3 to store the data, and Lambda to do the processing.\n",
    "\n",
    "The seismic data is streamed from S3 without the need to duplicate any portion of it. This is ideal for situations where you need to work with large (+100GB) SEGY files that cannot always be loaded into memory and to also avoid the cost of making a duplicate on a local filesystem. S3 allows you to read specific bytes of data from the file allowing for reading and processing of the traces you need. Lambda will allow you to parallel process the data and with no need to provision any hardware, you only pay for the time it takes to perform the calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import struct\n",
    "import pickle\n",
    "import botocore\n",
    "import array as arr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from struct import Struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Below are some custom fuctions that load SEGY revision 1 data.  They will aid in decifering the SEGY format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the text header block\n",
    "def DecodeTextHeader(text_header_raw):\n",
    "    text_header = text_header_raw.decode('cp500')\n",
    "    text_header = text_header.replace(\"C 1 \", \"\\nC 1 \")\n",
    "    text_header = text_header.replace(\"C 2 \", \"\\nC 2 \")\n",
    "    text_header = text_header.replace(\"C 3 \", \"\\nC 3 \")\n",
    "    text_header = text_header.replace(\"C 4 \", \"\\nC 4 \")\n",
    "    text_header = text_header.replace(\"C 5 \", \"\\nC 5 \")\n",
    "    text_header = text_header.replace(\"C 6 \", \"\\nC 6 \")\n",
    "    text_header = text_header.replace(\"C 7 \", \"\\nC 7 \")\n",
    "    text_header = text_header.replace(\"C 8 \", \"\\nC 8 \")\n",
    "    text_header = text_header.replace(\"C 9 \", \"\\nC 9 \")\n",
    "    text_header = text_header.replace(\"C10 \", \"\\nC10 \")\n",
    "    text_header = text_header.replace(\"C11 \", \"\\nC11 \")\n",
    "    text_header = text_header.replace(\"C12 \", \"\\nC12 \")\n",
    "    text_header = text_header.replace(\"C13 \", \"\\nC13 \")\n",
    "    text_header = text_header.replace(\"C14 \", \"\\nC14 \")\n",
    "    text_header = text_header.replace(\"C15 \", \"\\nC15 \")\n",
    "    text_header = text_header.replace(\"C16 \", \"\\nC16 \")\n",
    "    text_header = text_header.replace(\"C17 \", \"\\nC17 \")\n",
    "    text_header = text_header.replace(\"C18 \", \"\\nC18 \")\n",
    "    text_header = text_header.replace(\"C19 \", \"\\nC19 \")\n",
    "    text_header = text_header.replace(\"C20 \", \"\\nC20 \")\n",
    "    text_header = text_header.replace(\"C21 \", \"\\nC21 \")\n",
    "    text_header = text_header.replace(\"C22 \", \"\\nC22 \")\n",
    "    text_header = text_header.replace(\"C23 \", \"\\nC23 \")\n",
    "    text_header = text_header.replace(\"C24 \", \"\\nC24 \")\n",
    "    text_header = text_header.replace(\"C25 \", \"\\nC25 \")\n",
    "    text_header = text_header.replace(\"C26 \", \"\\nC26 \")\n",
    "    text_header = text_header.replace(\"C27 \", \"\\nC27 \")\n",
    "    text_header = text_header.replace(\"C28 \", \"\\nC28 \")\n",
    "    text_header = text_header.replace(\"C29 \", \"\\nC29 \")\n",
    "    text_header = text_header.replace(\"C30 \", \"\\nC30 \")\n",
    "    text_header = text_header.replace(\"C31 \", \"\\nC31 \")\n",
    "    text_header = text_header.replace(\"C32 \", \"\\nC32 \")\n",
    "    text_header = text_header.replace(\"C33 \", \"\\nC33 \")\n",
    "    text_header = text_header.replace(\"C34 \", \"\\nC34 \")\n",
    "    text_header = text_header.replace(\"C35 \", \"\\nC35 \")\n",
    "    text_header = text_header.replace(\"C36 \", \"\\nC36 \")\n",
    "    text_header = text_header.replace(\"C37 \", \"\\nC37 \")\n",
    "    text_header = text_header.replace(\"C38 \", \"\\nC38 \")\n",
    "    text_header = text_header.replace(\"C39 \", \"\\nC39 \")\n",
    "    text_header = text_header.replace(\"C40 \", \"\\nC40 \")\n",
    "    \n",
    "    return text_header\n",
    "\n",
    "# Decode the binary header block\n",
    "def DecodeBinHeader(bin_header_raw):\n",
    "    bin_header = {}\n",
    "\n",
    "    bin_header['job_id']                  = int.from_bytes(bin_header_raw[0:4], byteorder='big', signed=False)\n",
    "    bin_header['line_no']                 = int.from_bytes(bin_header_raw[4:8], byteorder='big', signed=False)\n",
    "    bin_header['reel_no']                 = int.from_bytes(bin_header_raw[8:12], byteorder='big', signed=False)\n",
    "    bin_header['data_traces']             = int.from_bytes(bin_header_raw[12:14], byteorder='big', signed=False)\n",
    "    bin_header['aux_traces']              = int.from_bytes(bin_header_raw[14:16], byteorder='big', signed=False)\n",
    "    bin_header['sample_interval']         = int.from_bytes(bin_header_raw[16:18], byteorder='big', signed=False)\n",
    "    bin_header['sample_interval_orig']    = int.from_bytes(bin_header_raw[18:20], byteorder='big', signed=False)\n",
    "    bin_header['samples_per_trace']       = int.from_bytes(bin_header_raw[20:22], byteorder='big', signed=False)\n",
    "    bin_header['samples_per_trace_orig']  = int.from_bytes(bin_header_raw[22:24], byteorder='big', signed=False)\n",
    "    bin_header['data_sample_format']      = int.from_bytes(bin_header_raw[24:26], byteorder='big', signed=False)\n",
    "    bin_header['ensemble_fold']           = int.from_bytes(bin_header_raw[26:28], byteorder='big', signed=False)\n",
    "    bin_header['trace_sorting']           = int.from_bytes(bin_header_raw[28:30], byteorder='big', signed=False)\n",
    "    bin_header['vert_sum_code']           = int.from_bytes(bin_header_raw[30:32], byteorder='big', signed=False)\n",
    "    bin_header['sweep_hz_start']          = int.from_bytes(bin_header_raw[32:34], byteorder='big', signed=False)\n",
    "    bin_header['sweep_hz_end']            = int.from_bytes(bin_header_raw[34:36], byteorder='big', signed=False)\n",
    "    bin_header['sweep_length']            = int.from_bytes(bin_header_raw[36:38], byteorder='big', signed=False)\n",
    "    bin_header['sweep_type']              = int.from_bytes(bin_header_raw[38:40], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_ch']          = int.from_bytes(bin_header_raw[40:42], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_taper_start'] = int.from_bytes(bin_header_raw[42:44], byteorder='big', signed=False)\n",
    "    bin_header['sweep_trace_taper_end']   = int.from_bytes(bin_header_raw[44:46], byteorder='big', signed=False)\n",
    "    bin_header['taper_type']              = int.from_bytes(bin_header_raw[46:48], byteorder='big', signed=False)\n",
    "    bin_header['data_traces_correlated']  = int.from_bytes(bin_header_raw[48:50], byteorder='big', signed=False)\n",
    "    bin_header['binary_gain_recovered']   = int.from_bytes(bin_header_raw[50:52], byteorder='big', signed=False)\n",
    "    bin_header['amp_recovery_method']     = int.from_bytes(bin_header_raw[52:54], byteorder='big', signed=False)\n",
    "    bin_header['measurement_system']      = int.from_bytes(bin_header_raw[54:56], byteorder='big', signed=False)\n",
    "    bin_header['impulse_sig_polarity']    = int.from_bytes(bin_header_raw[56:58], byteorder='big', signed=False)\n",
    "    bin_header['vib_polarity']            = int.from_bytes(bin_header_raw[58:60], byteorder='big', signed=False)\n",
    "    bin_header['unassigned']              = int.from_bytes(bin_header_raw[60:300], byteorder='big', signed=False)\n",
    "    bin_header['segy_format']             = int.from_bytes(bin_header_raw[300:302], byteorder='big', signed=False)\n",
    "    bin_header['fixed_length_flag']       = int.from_bytes(bin_header_raw[302:304], byteorder='big', signed=False)\n",
    "    bin_header['extended_text_header_no'] = int.from_bytes(bin_header_raw[304:306], byteorder='big', signed=False)\n",
    "    bin_header['unassigned2']             = int.from_bytes(bin_header_raw[306:400], byteorder='big', signed=False)\n",
    "    \n",
    "    return bin_header\n",
    "\n",
    "# Display the binary header data.\n",
    "def PrintBinHeader(bin_header):\n",
    "    print(\"job_id                  = \", bin_header['job_id']                 )\n",
    "    print(\"line_no                 = \", bin_header['line_no']                )\n",
    "    print(\"reel_no                 = \", bin_header['reel_no']                )\n",
    "    print(\"data_traces             = \", bin_header['data_traces']            )\n",
    "    print(\"aux_traces              = \", bin_header['aux_traces']             )\n",
    "    print(\"sample_interval         = \", bin_header['sample_interval']        )\n",
    "    print(\"sample_interval_orig    = \", bin_header['sample_interval_orig']   )\n",
    "    print(\"samples_per_trace       = \", bin_header['samples_per_trace']      )\n",
    "    print(\"samples_per_trace_orig  = \", bin_header['samples_per_trace_orig'] )\n",
    "    print(\"data_sample_format      = \", bin_header['data_sample_format']     )\n",
    "    print(\"ensemble_fold           = \", bin_header['ensemble_fold']          )\n",
    "    print(\"trace_sorting           = \", bin_header['trace_sorting']          )\n",
    "    print(\"vert_sum_code           = \", bin_header['vert_sum_code']          )\n",
    "    print(\"sweep_hz_start          = \", bin_header['sweep_hz_start']         )\n",
    "    print(\"sweep_hz_end            = \", bin_header['sweep_hz_end']           )\n",
    "    print(\"sweep_length            = \", bin_header['sweep_length']           )\n",
    "    print(\"sweep_type              = \", bin_header['sweep_type']             )\n",
    "    print(\"sweep_trace_ch          = \", bin_header['sweep_trace_ch']         )\n",
    "    print(\"sweep_trace_taper_start = \", bin_header['sweep_trace_taper_start'])\n",
    "    print(\"sweep_trace_taper_end   = \", bin_header['sweep_trace_taper_end']  )\n",
    "    print(\"taper_type              = \", bin_header['taper_type']             )\n",
    "    print(\"data_traces_correlated  = \", bin_header['data_traces_correlated'] )\n",
    "    print(\"binary_gain_recovered   = \", bin_header['binary_gain_recovered']  )\n",
    "    print(\"amp_recovery_method     = \", bin_header['amp_recovery_method']    )\n",
    "    print(\"measurement_system      = \", bin_header['measurement_system']     )\n",
    "    print(\"impulse_sig_polarity    = \", bin_header['impulse_sig_polarity']   )\n",
    "    print(\"vib_polarity            = \", bin_header['vib_polarity']           )\n",
    "    print(\"unassigned              = \", bin_header['unassigned']             )\n",
    "    print(\"segy_format             = \", bin(bin_header['segy_format'])[2:]   )    \n",
    "    print(\"fixed_length_flag       = \", bin_header['fixed_length_flag']      )\n",
    "    print(\"extended_text_header_no = \", bin_header['extended_text_header_no'])\n",
    "    print(\"unassigned2             = \", bin_header['unassigned2']            )\n",
    "    \n",
    "# Decode a binary trade header.\n",
    "def DecodeTraceHeader(trace_header_raw):\n",
    "    trace_header = {}\n",
    "    trace_header['trace_seq_no_all']            = int.from_bytes(trace_header_raw[0:4], byteorder='big', signed=False)\n",
    "    trace_header['trace_seq_no_file']           = int.from_bytes(trace_header_raw[4:8], byteorder='big', signed=False)\n",
    "    trace_header['field_record_no_orig']        = int.from_bytes(trace_header_raw[8:12], byteorder='big', signed=False)\n",
    "    trace_header['trace_no_field_orig']         = int.from_bytes(trace_header_raw[12:16], byteorder='big', signed=False)\n",
    "    trace_header['energy_source_point_no']      = int.from_bytes(trace_header_raw[16:20], byteorder='big', signed=False)\n",
    "    trace_header['ensemble_no']                 = int.from_bytes(trace_header_raw[20:24], byteorder='big', signed=False)\n",
    "    trace_header['ensemble_trace_no']           = int.from_bytes(trace_header_raw[24:28], byteorder='big', signed=False)\n",
    "    trace_header['trace_id']                    = int.from_bytes(trace_header_raw[28:30], byteorder='big', signed=False)\n",
    "    trace_header['sum_vertical_traces']         = int.from_bytes(trace_header_raw[30:32], byteorder='big', signed=False)\n",
    "    trace_header['sum_horizontal_traces']       = int.from_bytes(trace_header_raw[32:34], byteorder='big', signed=False)\n",
    "    trace_header['data_use']                    = int.from_bytes(trace_header_raw[34:36], byteorder='big', signed=False)\n",
    "    trace_header['distance_from_source_center'] = int.from_bytes(trace_header_raw[36:40], byteorder='big', signed=False)\n",
    "    # ... incomplete\n",
    "    trace_header['group_x']                     = int.from_bytes(trace_header_raw[80:84], byteorder='big', signed=False)\n",
    "    trace_header['group_y']                     = int.from_bytes(trace_header_raw[84:88], byteorder='big', signed=False)\n",
    "    trace_header['coord_units']                 = int.from_bytes(trace_header_raw[88:90], byteorder='big', signed=False)\n",
    "    trace_header['trace_samples']               = int.from_bytes(trace_header_raw[114:116], byteorder='big', signed=False)\n",
    "    trace_header['sample_interval']             = int.from_bytes(trace_header_raw[116:118], byteorder='big', signed=False)\n",
    "    trace_header['gain_type']                   = int.from_bytes(trace_header_raw[118:120], byteorder='big', signed=False)\n",
    "    trace_header['CDP_X']                       = int.from_bytes(trace_header_raw[180:184], byteorder='big', signed=False)\n",
    "    trace_header['CDP_Y']                       = int.from_bytes(trace_header_raw[184:188], byteorder='big', signed=False)\n",
    "    trace_header['inline']                      = int.from_bytes(trace_header_raw[188:192], byteorder='big', signed=False)\n",
    "    trace_header['xline']                       = int.from_bytes(trace_header_raw[192:196], byteorder='big', signed=False)\n",
    "    trace_header['trace_unit']                  = int.from_bytes(trace_header_raw[202:204], byteorder='big', signed=False) \n",
    "    trace_header['inline_custom']               = int.from_bytes(trace_header_raw[232:236], byteorder='big', signed=False)\n",
    "    trace_header['xline_custom']                = int.from_bytes(trace_header_raw[236:240], byteorder='big', signed=False)\n",
    "\n",
    "    return trace_header\n",
    "    \n",
    "# Display the trade header data.\n",
    "def PrintTraceHeaders(trace_header):\n",
    "    print(\"trace_seq_no_all            = \", trace_header['trace_seq_no_all'])\n",
    "    print(\"trace_seq_no_file           = \", trace_header['trace_seq_no_file'])\n",
    "    print(\"field_record_no_orig        = \", trace_header['field_record_no_orig'])\n",
    "    print(\"trace_no_field_orig         = \", trace_header['trace_no_field_orig'])\n",
    "    print(\"energy_source_point_no      = \", trace_header['energy_source_point_no'])\n",
    "    print(\"ensemble_no                 = \", trace_header['ensemble_no'])\n",
    "    print(\"ensemble_trace_no           = \", trace_header['ensemble_trace_no'])\n",
    "    print(\"trace_id                    = \", trace_header['trace_id'])\n",
    "    print(\"sum_vertical_traces         = \", trace_header['sum_vertical_traces'])\n",
    "    print(\"sum_horizontal_traces       = \", trace_header['sum_horizontal_traces'])\n",
    "    print(\"data_use                    = \", trace_header['data_use'])\n",
    "    print(\"distance_from_source_center = \", trace_header['distance_from_source_center'])\n",
    "    # ... incomplete\n",
    "    print(\"group_x                     = \", trace_header['group_x'])\n",
    "    print(\"group_y                     = \", trace_header['group_y'])\n",
    "    print(\"coord_units                 = \", trace_header['coord_units'])\n",
    "    print(\"trace_samples               = \", trace_header['trace_samples'])\n",
    "    print(\"sample_interval             = \", trace_header['sample_interval'])\n",
    "    print(\"gain_type                   = \", trace_header['gain_type'])\n",
    "    print(\"CDP_X                       = \", trace_header['CDP_X'])          \n",
    "    print(\"CDP_Y                       = \", trace_header['CDP_Y'])          \n",
    "    print(\"inline                      = \", trace_header['inline'])         \n",
    "    print(\"xline                       = \", trace_header['xline'])          \n",
    "    print(\"trace_unit                  = \", trace_header['trace_unit'])     \n",
    "    print(\"inline_custom               = \", trace_header['inline_custom'])         \n",
    "    print(\"xline_custom                = \", trace_header['xline_custom'])          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SEGY\n",
    "We will load the SEGY file from S3 and create a StreamingBody object we can use to stream data into the notebook.  We do not need to copy the file locally, we will stream data bit-by-bit as needed.  \n",
    "\n",
    "You should modify these variables to point towards your own S3 bucket and SEGY file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the 100GB SEGY data\n",
    "\n",
    "source_bucket   = 'geophysics-on-cloud'                 # S3 bucket name with input data\n",
    "source_folder   = 'poseidon/seismic/segy'               # Folder path\n",
    "source_filename = 'psdn11_TbsdmF_full_w_AGC_Nov11.segy' # Filename\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "segy_obj = s3.Object(source_bucket, f\"{source_folder}/{source_filename}\")\n",
    "segy_stream = segy_obj.get()['Body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Headers\n",
    "Lets read the text and binary headers of the SEGY file.  We are assuming the file follows the SEGY Rev1 standard, which is the case for this file.\n",
    "\n",
    "In this format, the first 3200 bytes are the text header, the next 400 are the binary headers, so we are saving it to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_header_raw = segy_stream.read(3200)\n",
    "bin_header_raw = segy_stream.read(400)\n",
    "\n",
    "text_header = DecodeTextHeader(text_header_raw)\n",
    "\n",
    "print(\"Text Header:\")\n",
    "print(text_header)\n",
    "\n",
    "bin_header = DecodeBinHeader(bin_header_raw)\n",
    "print(\"\\nBinary Header:\")\n",
    "PrintBinHeader(bin_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traces\n",
    "After the file headers, the remainder of the SEGY consists of pairs of trace header blocks and the actual trace values in sequence.\n",
    "\n",
    "Lets read one of the binary trace headers.  This dataset contains a handful of traces without amplitudes, so we will skip a few traces at first to check to see the data is being read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip a 1000 traces, as the first handful do not have any usable data.\n",
    "skip_no = 1000\n",
    "\n",
    "for x in range(0, skip_no):\n",
    "    trace_header_raw = segy_stream.read(240)\n",
    "    trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "    trace_raw = segy_stream.read(trace_header['trace_samples']*4)\n",
    "\n",
    "trace_header_raw = segy_stream.read(240)\n",
    "trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "PrintTraceHeaders(trace_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace Data\n",
    "SEGY stores the trace amplitudes in different formats, outlined by the binary header \"data_sample_format\".  \n",
    "\n",
    "In this file the trace data is in IEEE 4-byte float, which is directly useable in Python.  We do not need to convert it, but only unpack it from the byte format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_raw = segy_stream.read(trace_header['trace_samples']*4)\n",
    "\n",
    "trace = []\n",
    "for x in range(4, trace_header['trace_samples']*4+4, 4):\n",
    "    trace.append(struct.unpack('>f', trace_raw[x-4:x])[0])\n",
    "\n",
    "# Lets plot the trace\n",
    "limits = np.amax(np.absolute(trace))\n",
    "plt.figure(figsize=(5, 10))\n",
    "plt.plot(trace, range(trace_header['trace_samples']), 'red')\n",
    "plt.xlim(-limits, limits)\n",
    "plt.ylim(trace_header['trace_samples'], 0)\n",
    "plt.axvline(0, c='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segy_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data on EC2/SageMaker Notebook\n",
    "\n",
    "First, for a benchmark, lets start processing here and see how long it would take on a single vCPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segy_stream = segy_obj.get()['Body']\n",
    "\n",
    "text_header_raw = segy_stream.read(3200)\n",
    "bin_header_raw = segy_stream.read(400)\n",
    "text_header = DecodeTextHeader(text_header_raw)\n",
    "bin_header = DecodeBinHeader(bin_header_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_amp_mean = {}\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through all the traces\n",
    "while True:\n",
    "    trace_header_raw = segy_stream.read(240)\n",
    "    trace_header = DecodeTraceHeader(trace_header_raw)\n",
    "    trace_raw = segy_stream.read(trace_header['trace_samples']*4)\n",
    "\n",
    "    trace = []\n",
    "    for x in range(4, trace_header['trace_samples']*4+4, 4):\n",
    "        trace.append(struct.unpack('>f', trace_raw[x-4:x])[0])\n",
    "    \n",
    "    trace_amp_mean[trace_header['trace_seq_no_all']] = np.mean(np.absolute(trace))\n",
    "    \n",
    "    if trace_header['trace_seq_no_all']%1000 == 0:\n",
    "        print(\"Trace #{} has a mean amplitude of {}, elapse time is {:.2f} seconds.\".format(\n",
    "                                        trace_header['trace_seq_no_all'], \n",
    "                                        trace_amp_mean[trace_header['trace_seq_no_all']], \n",
    "                                        time.time() - start_time))\n",
    "        \n",
    "    if trace_header['trace_seq_no_all'] > 5000:\n",
    "        print(\"Stopping here.\")\n",
    "        break\n",
    "\n",
    "segy_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Out\n",
    "\n",
    "Processing the SEGY file with a single machine is quite slow.  It would take about 4 hours to complete on this T2.Medium instance.  We could scale up and use a more powerful instance type, but then we are paying per hour, even if we are not using the full compute capacity.  Futhermore will will have to code the parallel processing.  It can quickly start becoming expensive to leave the machines running and the code complexity will increase.  The flow would look like the image below:\n",
    "\n",
    "\n",
    "![title](images/Page-1.png)\n",
    "\n",
    "\n",
    "So lets leverage the power of the cloud to scale out and distribute the processing workload with on-demand processing, Lambda!  Lets split this workload and send it to 1000 Lambdas that can run in parallel.  We do not need to send any seismic data to Lambda, we will simply tell each one where to find the file on S3 and which portion of the file to load.\n",
    "\n",
    "\n",
    "![title](images/Page-2.png)\n",
    "\n",
    "\n",
    "If you are following along with the GitHub repo and using the CloudFormation template, put in the bucket name you created when deploying the template.\n",
    "\n",
    "There is a Lambda function that contains the same calculations we performed above called [SegyBatchProcessMeanAmp](https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/SegyBatchProcessMeanAmp).  If you want, go check it out in the AWS Console.  It was deployed with the CF template.\n",
    "Lets get things setup up below.  Adjust variables below:\n",
    "\n",
    "**lambda_name** - Visit the Lambda page in the AWS Console and see what the final name is for your. \n",
    "\n",
    "**results_bucket** - Rename to the S3 bucket you created with the CloudFormation template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_name        = \"Seismic-Lambdas-4-SegyMeanAmpLambda-15NCECNNVG6X2\"    # Name of the Lambda function to invoke\n",
    "results_bucket     = \"seismic-results-vavourak-4\"                           # Bucket to use\n",
    "mean_amp_folder    = \"temp-trace-bundles-poseidon-mean-amp\"                 # Subfolder to place the calculation results\n",
    "concurrent_lambdas = 1000                                                   # Number of Lambdas to invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split up the SEGY file into byte ranges and invoke 1000 Lambda functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get S3 object\n",
    "segy_obj = s3.Object(source_bucket, f\"{source_folder}/{source_filename}\")\n",
    "\n",
    "# Define some needed variables based off the above parameters\n",
    "start_time = time.time()\n",
    "header_size = 3600\n",
    "trace_header_size = 240\n",
    "trace_size = bin_header['samples_per_trace'] * 4\n",
    "trace_size_with_headers = trace_size + trace_header_size\n",
    "filesize = segy_obj.content_length\n",
    "trace_count = int((filesize - 3600) / trace_size_with_headers)\n",
    "bundle_size = round(trace_count/concurrent_lambdas)\n",
    "lambda_counter = 0\n",
    "\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "results_file_list = [] # Lets keep track of the output file names, so we can grab them later\n",
    "\n",
    "print(f\"Total traces in file: {trace_count}\")\n",
    "print(f\"Traces per Lambda for {concurrent_lambdas} concurrency (not rounded): {trace_count/concurrent_lambdas}\")\n",
    "print(f\"Traces per Lambda, rounded up: {round(trace_count/concurrent_lambdas+0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the trace bundle information over to Lambda\n",
    "for bundle in range(0, int(trace_count), bundle_size):\n",
    "    lambda_counter = lambda_counter + 1\n",
    "    bytes_start = bundle * trace_size_with_headers + header_size\n",
    "    bytes_stop = (bundle + bundle_size) * trace_size_with_headers + header_size - 1\n",
    "    print(f\"Lambda #{lambda_counter}, bundled traces: {bundle}-{bundle+bundle_size}, bytes: {bytes_start}-{bytes_stop}.\")\n",
    "    \n",
    "    # Build the message for the Lambda to find the files\n",
    "    payload = {\n",
    "        \"bucket_in\"          : source_bucket,\n",
    "        \"folder_in\"          : source_folder,\n",
    "        \"filename_in\"        : source_filename,\n",
    "        \"bucket_out\"         : results_bucket,\n",
    "        \"folder_out\"         : mean_amp_folder,\n",
    "        \"bytes_start\"        : bytes_start,\n",
    "        \"bytes_stop\"         : bytes_stop,\n",
    "        \"use_custom_lines\"   : 0,\n",
    "        \"data_sample_format\" : bin_header['data_sample_format']\n",
    "    }\n",
    "\n",
    "    # Invoke the Lambda SegyBatchProcessMeanAmp\n",
    "    response = lambda_client.invoke(FunctionName=lambda_name,\n",
    "                                    InvocationType='Event',\n",
    "                                    Payload=json.dumps(payload))\n",
    "\n",
    "    results_file_list.append(f\"{mean_amp_folder}/{source_filename}.{bytes_start}-{bytes_stop}.pkl\")\n",
    "    \n",
    "print(\"Done!  Elapse time to gather traces and send to Lambda: {:0.2f} seconds, now waiting a bit for processing to complete.\".format(time.time() - start_time))\n",
    "\n",
    "time.sleep(200)      # Waiting before carrying on next steps, to allow time for Lambda to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a 1000 Lambdas, taking roughly 170 seconds each, 46 seconds to start them, our total time is about 4 minutes.  The cost to perform this calculation on this 100GB file is about $0.35.  As it takes 46 seconds to invoke the Lambdas, there is some room for optimization here to invoke them faster.  Having more Lambda concurrency available will help further, as each one can process a smaller portion of the file.  \n",
    "\n",
    "### Load Results\n",
    "The Lambdas should be done by now. Lets load in the results from S3.  Due to limitations in matplotlib's ability to display large datasets, we will only load 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "traces = []\n",
    "trace_mean_amp = []\n",
    "trace_x = []\n",
    "trace_y = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through the files\n",
    "#for x in range(0, len(results_file_list)):\n",
    "for x in range(0, 1000, 10):\n",
    "    print(\"Reading file: \", results_file_list[x])\n",
    "    \n",
    "    # Get file from S3, convert from Pickle format\n",
    "    object = s3_client.get_object(Bucket=results_bucket, Key=results_file_list[x])\n",
    "    serializedObject = object['Body'].read()\n",
    "    trace_bundle_temp = pickle.loads(serializedObject)\n",
    "    \n",
    "    #for y in range(0, len(trace_bundle_temp)):\n",
    "    #    traces.append(trace_bundle_temp[y])\n",
    "    # Split the tuple [1,2,3] into seperate variables for easier use\n",
    "    for y in range(0, len(trace_bundle_temp)):\n",
    "        trace_mean_amp.append(trace_bundle_temp[y][0])\n",
    "        trace_x.append(trace_bundle_temp[y][1])\n",
    "        trace_y.append(trace_bundle_temp[y][2])\n",
    "\n",
    "print(\"Number of traces loaded: {}, elapsed time: {:0.2f} seconds.\".format(len(trace_mean_amp), time.time() - start_time))\n",
    "print(\"Elapsed time: {:0.2f} seconds.\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well will cull any traces that lack data.  There are a handful that do not have any amplitude and will make it more complicated displaying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_culled = []\n",
    "trace_x_culled = []\n",
    "trace_y_culled = []\n",
    "\n",
    "print('Number of traces before:', len(trace_mean_amp))\n",
    "\n",
    "# Go through every trace and cull any traces without an amplitude\n",
    "counter = 0\n",
    "for i in range(0, len(trace_mean_amp)):\n",
    "    if trace_mean_amp[i] > 1000:\n",
    "        trace_culled.append(trace_mean_amp[i])\n",
    "        trace_x_culled.append(trace_x[i])\n",
    "        trace_y_culled.append(trace_y[i])\n",
    "\n",
    "print('Number of traces after: ', len(trace_culled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets map out the mean amplitudes using a Matplotlib scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(trace_x_culled, trace_y_culled, c=trace_culled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We have successfully performed a mathematical calculation on a 100GB seismic SEGY file without having to spin up or down a single computer.  All at the cost of $0.35 and 4 minutes of processing time.  This process is scalable and repeatable.  It can be expanded upon and automated with AWS Step Functions to orchestrate multiple steps and SageMaker to perform ML inference on the data.  Lambda concurrency can even be increased from the default 1000 to hundreds of thousands, greatly increasing performance.  \n",
    "\n",
    "Of course, this calculation is not at the complexity of Full Waveform Inversion or other computations that require high performance computing (HPC).  However, if those calculations can be adjusted to work with Lambda (up to 6 vCPU, 10GB RAM, 15 minute limit), a huge amount of undifferentiated heavy lifting can be removed and offer an alternative to the complixity of scheduling thousands of EC2 spot instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Lets clean up the files we generated, so that we can delete the CloudFormation stack without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(results_file_list)):\n",
    "    _ = s3_client.delete_object(Bucket=results_bucket, Key=results_file_list[x])\n",
    "    print(\"Deleting: {}\".format(results_file_list[x]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
